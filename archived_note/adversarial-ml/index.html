<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=0.8"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.3.0 for Hugo"><meta name=google-site-verification content="Y6phf4VZM9HednfYy-pEi3-KtU6uqI75tPeeinWlOYs"><meta name=baidu-site-verification content="code-7BynQM9TAs"><meta name=author content="Wenda Chu 储闻达"><meta name=description content="My learning and research notes of Machine Learning in the adversarial setting."><link rel=alternate hreflang=en-us href="index.html"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><script src="../../js/mathjax-config.js"></script><link rel=stylesheet href="../../css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css" media=print onload="this.media='all'"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous media=print onload="this.media='all'"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload="this.media='all'"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload="this.media='all'" disabled><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Cutive+Mono&family=Lora:wght@400;700&family=Roboto:wght@400;700&display=swap&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Cutive+Mono&family=Lora:wght@400;700&family=Roboto:wght@400;700&display=swap&display=swap" media=print onload="this.media='all'"><link rel=stylesheet href="../../css/wowchemy.ed2da72110e60110d27da0704ff033d1.css"><link rel=manifest href="../../manifest.webmanifest"><link rel=icon type=image/png href="../../media/icon_hud9f11bce4f3a2a4889ae0de212996427_55561_32x32_fill_lanczos_center_2.png"><link rel=apple-touch-icon type=image/png href="../../media/icon_hud9f11bce4f3a2a4889ae0de212996427_55561_180x180_fill_lanczos_center_2.png"><link rel=canonical href="index.html"><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Wenda Chu"><meta property="og:url" content="https://chuwd19.github.io/archived_note/adversarial-ml/"><meta property="og:title" content="Adversarial Machine Learning | Wenda Chu"><meta property="og:description" content="My learning and research notes of Machine Learning in the adversarial setting."><meta property="og:image" content="https://chuwd19.github.io/media/icon_hud9f11bce4f3a2a4889ae0de212996427_55561_512x512_fill_lanczos_center_2.png"><meta property="twitter:image" content="https://chuwd19.github.io/media/icon_hud9f11bce4f3a2a4889ae0de212996427_55561_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2021-10-18T00:00:00+00:00"><meta property="article:modified_time" content="2021-10-18T00:00:00+00:00"><title>Adversarial Machine Learning | Wenda Chu</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=2c910681fcb4c736cff7a67c4bb52111><script src="../../js/wowchemy-init.min.8988fb2a4bba758785868cfcb5244555.js"></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href="index.html#" aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href="../../index.html">Wenda Chu</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href="../../index.html">Wenda Chu</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href="../../index.html#about"><span>Home</span></a></li><li class=nav-item><a class=nav-link href="../../index.html#featured"><span>Publications</span></a></li><li class=nav-item><a class=nav-link href="../../index.html#projects"><span>Projects</span></a></li><li class=nav-item><a class=nav-link href="../../index.html#contact"><span>Contact</span></a></li><li class=nav-item><a class=nav-link href="../../note.html"><span>Notes</span></a></li><li class=nav-item><a class=nav-link href="../../life.html"><span>Life</span></a></li><li class=nav-item><a class=nav-link href="../../uploads/resume.pdf"><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href="index.html#" aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href="index.html#" class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href="index.html#" class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href="index.html#" class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href="index.html#" class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Adversarial Machine Learning</h1><p class=page-subtitle>My learning and research notes of Machine Learning in the adversarial setting.</p><div class=article-metadata><div><span class=author-highlighted>Wenda Chu 储闻达</span></div><span class=article-date>Oct 18, 2021</span>
<span class=middot-divider></span><span class=article-categories><i class="fas fa-folder mr-1"></i><a href="../../category/notes/index.html">Notes</a></span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href="Adversarial-ML.pdf" target=_blank rel=noopener>PDF</a></div></div><div class=article-container><div class=article-style><h2 id=centernotes-on-adversarial-machine-learningcenter><center>Notes on Adversarial Machine Learning</center></h2><h3 id=1--formalize-adversarial-attack>1 Formalize Adversarial Attack</h3><h5 id=explorative-attacks-vs-causative-attack>Explorative Attacks vs. Causative Attack</h5><ul><li><p><strong>Explorative attacks</strong>: the attacker influences only the evaluation data.</p></li><li><blockquote><p>The attempts to passively circumvent the learning mechanism to explore blind spots in the learner</p><p>&mldr; to craft intrusions so as to evade the classifier without direct influence over the classifier itself</p></blockquote></li><li><p><strong>Causative attacks</strong>: the attacker attempts to hack the training data as well.</p></li><li><p>In the following survey, an adversary is usually assumed to be <strong>explorative</strong>.</p></li></ul><h5 id=adversarys-goal>Adversary&rsquo;s Goal</h5><ul><li><p>For an input $I_c\in \mathbb R^m$, find a small perturbation $\rho$ to force a classifier $\mathcal C$ to label $\ell$. ((<a href=https://arxiv.org/abs/1312.6199 target=_blank rel=noopener>Szegedy et al. 2014</a>)
$$
\min |\rho|, s.t.\mathcal C(I_c+\rho) = \ell
$$</p></li><li><p>Another definition is to minimize the loss function on label $\ell$, with perturbation $\rho$ subject to some restriction.
$$
\min_{\rho\in \Delta}\mathcal L(I_c +\rho, \ell)
$$</p><ul><li><strong>Targeted</strong>: Fool the classifier to a specific label $\ell$</li><li><strong>Untargeted</strong>: Any $\ell$ different from the origin class suffices.</li></ul></li></ul><h5 id=adversarys-strength>Adversary&rsquo;s Strength</h5><ul><li><p>An adversary may have access to some of the knowledges below:</p><ul><li><p>Training dataset</p></li><li><p>The feature representation of a sample (a vector in the feature space)</p></li><li><p>Learning algorithm of the model (e.g. architecture of a neural network)</p></li><li><p>The whole trained model with parameters</p></li><li><p>Output of the learner</p></li></ul></li><li><p>If an attack only requires input-output behavior of the model, it is referred to as a <strong>black box attack</strong>. (In some looser definition, the output of loss function is also accessible.)</p></li><li><p>Otherwise, it is a <strong>white box attack</strong>.</p></li></ul><h3 id=2--typical-attacks-for-classification>2 Typical Attacks for Classification</h3><h5 id=box-constrained-l-bfgs-szegedy-et-al-2014httpsarxivorgabs13126199>Box-constrained L-BFGS (<a href=https://arxiv.org/abs/1312.6199 target=_blank rel=noopener>Szegedy et al. 2014</a>)</h5><ul><li>The origin goal (1) of an adversary is generally too hard a problem for optimization. It is helpful to transform it into the following form:</li></ul><p>$$
\rho_c^* = \min_\rho c|\rho| + \mathcal L(I_c+\rho, \ell), s.t. I_c + \rho\in[0,1]^m
$$</p><ul><li>We need to find the minimal parameter $c>0$, such that $\mathcal C(I_c + \rho_c^*) = \ell$. The optimum of problem (3) can be sought using L-BFGS. It is proved that two optimization problem (1) and (3) yield same results under convex losses.</li><li>Szegedy&rsquo;s paper also suggests an upper bound on unstability only by network architecture. This is done by inspecting the upper Lipschitz constant of each layer: if layer $k$ is $L_k$-Lipschitz, the whole network would be $L = \prod_{k=1}^K L_k$ Lipschitz:</li></ul><p>$$
|\phi(I_c) - \phi(I_c + \rho)||\leq L|r|
$$</p><ul><li>This bound is usually too loose to be meaningful, but according to Szegedy, it implies that regularization that penalizing each upper Lipschitz bound might help the robustness of the network.</li></ul><h5 id=fgsm-goodfellow-et-al-2015httpsarxivorgabs14126572>FGSM (<a href=https://arxiv.org/abs/1412.6572 target=_blank rel=noopener>Goodfellow et al. 2015</a>)</h5><ul><li><p>A <strong>linear</strong> and <strong>one-shot</strong> perturbation:
$$
\rho = \epsilon \cdot sign(\nabla_x \mathcal L(\theta,x,y))
$$</p></li><li><p>In this paper, it is shown that:</p><ul><li>Linear models are sufficient for the existence of adversarial attacks, since small perturbation results in a huge variation due to high dimensionality.</li><li>It is hypothesized that it is linearity instead of non-linearity that makes models vulnerable.</li></ul></li><li><p>The computational efficiency of one-shot perturbation enables adversarial training.</p></li></ul><h5 id=iterative-methods-kurakin-et-al-2017httpsarxivorgabs160702533>Iterative Methods (<a href=https://arxiv.org/abs/1607.02533 target=_blank rel=noopener>Kurakin et al. 2017</a>)</h5><ul><li>Basic iterative method: this is essentially a PGD of $\ell^{\infty}$ ball.</li></ul><p>$$
I_\rho^{(i+1)} = Clip_\epsilon [I_\rho^{(i)} + \alpha sign(\nabla \mathcal L(\theta, I_\rho^{(i)}, \ell))]
$$</p><ul><li>Least-likely-class iterative method:</li></ul><p>$$
I_\rho^{(i+1)} = Clip_\epsilon[I_\rho^{(i)}-\alpha sign(\nabla \mathcal L(\theta, I_\rho^{(i)}), \ell_{target})]
$$</p><ul><li>where $\ell_{target}$ is the least likely class of prediction.</li></ul><h5 id=jacobian-based-saliency-map-attack>Jacobian based Saliency Map Attack</h5><ul><li>$\ell_0$ norm attack (not read yet)</li></ul><h5 id=one-pixel-attack>One Pixel Attack</h5><ul><li>Applies <strong>differential evolution</strong> to generate adversarial examples</li><li>Black box attack: Requires only the predicted likelihood vector, but not the loss function or its gradient.</li></ul><h5 id=carlini-and-wagner-attacks>Carlini and Wagner Attacks</h5><ul><li><p>Find objective functions $f$, such that
$$
f(I_c + \rho) \leq 0 \text{ iff } \mathcal C(I_c + \rho) = \ell
$$
which enables an alternative optimization formulation:
$$
\min |\rho| + c\cdot f(I_c + \rho),\ \mathrm{s.t.}\ I_c +\rho\in [0,1]^n
$$</p></li><li><p>An efficient objective function $f$ is found to be
$$
f(x) = \max(\max_{i\neq t} Z(x)_i - Z(x)_t, -\kappa),
$$
where the classifier is assumed to be:
$$
\mathcal C(x) = Softmax(Z(x)).
$$
The parameter $\kappa\geq 0$ forces an adversary to find adversarial examples of higher confidence. It is shown that $\kappa$ is positively correlated to the transferability of the adversarial examples found.</p></li><li><p>Yet another trick is used for the box constraints. Let $x = \frac{1}{2}(\tanh(w)+1)$, so $x$ satisfies $x\in [0,1]$ automatically.</p></li></ul><h3 id=3--transferability>3 Transferability</h3><ul><li><p><strong>Transferability:</strong> the ability of an adversarial example to remain effective on differently trained models.</p></li><li><p>A more careful definition (<a href=https://arxiv.org/abs/1605.07277 target=_blank rel=noopener>Papernot et al. 2016</a>):</p><ul><li><strong>Intra-technique</strong> transferability: consider models trained with the same technique but different parameter
initializations or datasets</li><li><strong>cross-technique</strong> transferability: consider models trained with different techniques</li></ul></li><li><p>Transferability empowers black-box attacks: to train a substitute model by querying the classifier as an oracle.</p></li><li><p>Several methods for data augmentation are proposed by Papernot et al.</p></li></ul><h5 id=universal-adversarial-perturbations-moosavi-dezfooli-et-al-2017httpsarxivorgabs161008401>Universal Adversarial Perturbations (<a href=https://arxiv.org/abs/1610.08401 target=_blank rel=noopener>Moosavi-Dezfooli et al. 2017</a>)</h5><ul><li>A perturbation is <strong>universal</strong> if:</li></ul><p>$$
\Pr_{I_c\sim S} (\mathcal C(I_c)\neq \mathcal C(I_c+\rho)) \geq 1-\delta,\ \mathrm{s.t.}|\rho|_p\leq\epsilon
$$</p><blockquote><p>For each image x in the validation set, we compute the adversarial perturbation vector $r(x)$&mldr; To quantify the correlation between different regions of the decision boundary of the classifier, we define the matrix $N = [\frac{r(x_1)}{|r(x_1)|_2} \dots \frac{r(x_n)}{|r(x_n)|_2}]$</p></blockquote><ul><li>The author compares the singular values of matrix $N$ with the singular values of a matrix with columns sampled randomly.</li><li>It is explained that a subspace of dimension $d^\prime \ll d$ containing most normal vectors to the decision boundary in regions
surrounding natural images.</li></ul><p><img src="figure/fig1.png" alt=avatar style=zoom:30%></p><h3 id=myth>Myth:</h3><ul><li><p>Why adversarial examples are so close to any input $x$?</p></li><li><p>Why adversarial examples looks like random noise?</p></li><li><p>Why training with mislabeling also yields models with great performance?</p></li><li><p>I listened to an online report made by <strong>Adi Shamir</strong></p></li><li><p>Assumptions:</p><ul><li>$k$-manifold assumption</li><li>The boundary of a classification network is only pushed to get close to the manifold during training</li><li>Claim: adversarial examples are nearly orthogonal to the manifold.</li><li>Test using generative model!</li></ul></li></ul><h3 id=4--defenses>4 Defenses</h3><h5 id=1-adversarial-training>1. Adversarial Training</h5><ul><li><p>Intuition: to argument the training data with perturbated examples.</p></li><li><p>Solving the min-max problem
$$
\min_\theta \sum_{(x,y)\in S}\max_{\rho\in \Delta} \mathcal L(\theta, x+\rho, y)
$$</p></li></ul><h5 id=2-to-detect-adversarial-examples>2. To Detect Adversarial Examples</h5><p><em><strong>On Detecting Adversarial Perturbations</strong></em> (<a href=https://arxiv.org/abs/1702.04267 target=_blank rel=noopener>Metzen et al. 2017</a>)</p><ul><li>Intuition: to train a small subnetwork for distinguishing genuine data from data containing adversarial perturbation</li><li>Train a normal classifier $\Rightarrow$ Generate adversarial examples $\Rightarrow$ Train the detector</li><li>Worst case: the adversary adapts to the detector:</li></ul><p>$$
I_\rho^{(i+1)} = Clip_\epsilon\left{I_\rho^{(i)} + \alpha\Big[(1-\sigma)\cdot sign(\nabla \mathcal L_{classify}(I_\rho^{(i)},\ell_{true}))+\sigma \cdot sign \big(\nabla \mathcal L_{detect}(I_\rho^{(i)})\big)\Big]\right}
$$</p><ul><li>where $\sigma$ allows the dynamic adversary to trade off these two objectives.</li><li>Apply the dynamic adversary and the detector alternately.</li></ul><p><em><strong>Detecting Adversarial Samples from Artifacts</strong></em> (<a href=https://arxiv.org/abs/1703.00410 target=_blank rel=noopener>Feinman et al. 2017</a>)</p><ul><li><p>A crucial drawback of Metzen&rsquo;s work: must be trained on generated adversarial examples</p></li><li><p>An intuition: high dimensional datasets are believed to lie on a ==low-dim manifold==; and the adversarial perturbations must push samples off the data manifold.</p></li><li><p><strong>Kernel Density estimation:</strong> Detect the points that are far away from the manifold.
$$
\hat f(x) = \frac{1}{|X_t|}\sum_{x_i\in X_t}k(\phi(x_i),\phi(x))
$$</p><ul><li><p>where $X_t$ is the set of training data with label $t$ (here $t$ means the predicted class).</p></li><li><p>$k(\cdot,\cdot)$ is the kernel function and $\phi(\cdot)$ maps input $x$ to its feature vector of the last hidden layer.</p></li><li><p>Another intuition: deeper layers provide more linear and unwrapped manifold.</p></li></ul></li><li><p><strong>Bayesian Neural Network Uncertainty:</strong> identify low-confidence regions by capturing &ldquo;==variance==&rdquo; of predictions</p><ul><li><p>Randomness is considered under dropouts and parameters are sampled for $T$ times.
$$
Var(y^*) \approx \frac{1}{T}\sum_{i=1}^T \hat y^* (x^*,W^t)^T\hat y^*(x^*,W^t) - \mathbb E(y^*)^T\mathbb E(y^*)
$$</p></li><li><p>where $y^* = f(x^*)$ is a prediction of test input $x*$.</p></li><li><p>It is shown that typical adversarial examples do have much different distributions on uncertainty.</p><p><img src="figure/fig2.png" alt=avatar style=zoom:35%></p></li></ul></li></ul><p><em><strong>Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods</strong></em> (<a href=https://arxiv.org/abs/1705.07263 target=_blank rel=noopener>Carlini et al. 2017</a>)</p><ul><li><p>Analyze 10 proposed defenses to ==detect== adversarial examples</p></li><li><p>Conclusion: all these defenses are <em>inefficient</em> when an adversary is aware the neural network is being secured with a given detection scheme; and some of the properties claimed for adversarial examples are only due to existing attack techniques.</p></li><li><p>The 10 defenses can be categorized:</p><ol><li>Train a secondary neural network for detection</li><li>Capture statistical properties</li><li>Perform input-normalization with randomization and blurring</li></ol></li><li><p>Break each defenses by:</p><ol><li><p>Secondary Detector:</p><ul><li>Treat &ldquo;malicious&rdquo; as a new label. Combine the detector and the classifier:</li></ul><p>$$
G(x)_i = \begin{cases}
Z_F(x)_i \qquad\qquad\qquad\qquad\qquad, \text{ if } i\leq N\<br>(Z_D(x)+1)\cdot \max_j Z_F(x)_j \qquad \text{if } i=N+1
\end{cases}
$$</p><p>where $Z_F, Z_D$ are logits of the classifier and detector, respectively.</p><ul><li>The detector marks &ldquo;malicious&rdquo; $\Leftrightarrow$ $Z_D(x)>0$ $\Leftrightarrow$ $\arg\max_i G(x_i) = N+1$</li></ul></li><li></li></ol></li></ul><h5 id=3-certified-defenses>3. Certified Defenses</h5><blockquote><p>Aim to &ldquo;provide rigorous guarantees of robustness against norm-bounded attacks&rdquo;</p></blockquote><p><em><strong>Certified Robustness to Adversarial Examples with Differential Privacy</strong></em> (<a href=https://arxiv.org/abs/1802.03471 target=_blank rel=noopener>Lecuyer et al. 2019</a>)</p><ul><li><p>Consider a classifier $\mathcal C(x)$ that outputs soft labels $(p_1,\dots, p_n)$, $\sum_{i = 1}^n p_i = 1$.</p></li><li><p>Suppose $\mathcal C(x)$ is $(\epsilon, \delta)$-DP, which implies $\mathbb E[p_i(x)] = e^{\epsilon}\mathbb E[p_i(x^\prime)]+ \delta$, for any $x,x^\prime$ such that $d(x,x^\prime) &lt; 1$.</p></li><li><p><strong>Main theorem:</strong> If $\mathcal C$ is $(\epsilon,\delta)$-DP, w.r.t. $\ell_p$ norm, and $\forall x, \exists k$, s.t.:</p><ul><li><p>$$
\mathbb E(\mathcal C_k(x)) \geq e^{2\epsilon} \max_{i\neq k} \mathbb E(\mathcal C_i(x)) + (1+e^\epsilon)\delta
$$</p></li><li><p>Then the classification model $y = \arg\max_{i=1}^n p_i$ is robust to attacks within the $\ell_p$ unit ball.</p></li></ul></li><li><p>This is different from traditional DP which uses $\ell_0$ norm for $d(x,x^\prime)$, and the definition of sensitivity must also be changed:
$$
\Delta_{p,q}^{(f)} = \max_{x\neq x^\prime} \frac{|f(x) - f(x^\prime)|_q}{|x-x^\prime|_p}
$$</p></li><li><p>The conclusion of DP can be applied to $p$ norm as well, namely: Laplacian mechanism works for bounded $\Delta_{p,1}$ and Gaussian mechanism works for $\Delta_{p,2}$. Moreover, as DP is immune to post-processing, we can add these noises at layer of the network!</p></li><li><p>Overall Scheme: Pre-noise layers + noise layer $\longrightarrow$ Post-noise layers</p></li><li><p>Only need to bound the sensitivity of pre-noise computation $x\mapsto g(x)$. This is done by transforming $g$ to $\tilde g$ with $\Delta_{p,q}^{(\tilde g)}\leq 1$.</p><ul><li>Techniques: Normalization, Projection SGD (Parseval networks, ==tbd==).</li></ul></li></ul><p><img src="figure/DP.png" alt=avatar style=zoom:50%></p><h3 id=5-restricted-threat-model-attacks>5 Restricted Threat Model Attacks</h3><p><em><strong>Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models</strong></em></p><ul><li><p>Story so far: gradient-based, score-based and transfer-based attacks</p></li><li><blockquote><p>Definition <strong>(Decision-based attacks):</strong> Direct attacks that solely rely on the final decision of the model</p></blockquote></li><li><p><strong>Method:</strong> Initialize with an adversarial input $x_0 = x^\prime,$ make random walk according to a &ldquo;proposal distribution&rdquo;, trying to reduce $|x_k - x^*|$.</p></li><li><p><strong>Performance:</strong> Requires (unsurprisingly) much more iterations of forward passes.</p></li></ul><p><img src="figure/Dicision_based.png" alt=avatar style=zoom:35%></p><h3 id=6-generative-models>6 Generative Models</h3><h5 id=61-variational-autoencoder-vae-background>6.1 Variational Autoencoder (VAE) Background</h5><ul><li>latent representation $z = Enc(x)$, and decoder/generator maps $z$ to $\hat x$. $\hat x = Dec(z)$.</li><li>VAE aims to learn a latent representation for posterior distribution $p(z|x)$. Maximize loss function (minimize KL divergence):</li></ul><p>$$
\begin{align}
\mathcal L_{VAE}&= \log p(x) - KL(q(z|x)|p(z|x))\notag\<br>&= \sum_z q(z|x) \log p(x) - \sum_z q(z|x) \log \frac{q(z|x)}{p(z|x)}\notag\<br>&= \mathbb E_{q(z|x)}[-\log q(z|x) + \log p(x,z)]\notag\<br>&= \sum_z q(z|x) \log \frac{p(z)}{q(z|x)} + \mathbb E_{q(z|x)} p(x|z)\notag\<br>&= -KL(q(z|x)|p(z)) + \mathbb E_{q(z|x)}p(x|z).
\end{align}
$$</p><h3 id=7-verifiably-robust-models>7 Verifiably Robust Models</h3><h5 id=71-interval-bound-propagation>7.1 Interval Bound Propagation</h5><ul><li>For input $x_0$ and logits $x_k$, we want worst case robustness in a neighbour of $x_0$:</li></ul><p>$$
(e_y - e_{y_{true}})^T\cdot z_k \leq 0,\ \forall z_0 \in \mathcal X(x_0). \label{verify}
$$</p><ul><li><p>where $z_k = logits(z_0)$.</p></li><li><p>Consider $z_k = \sigma(h(z_{k-1}))$ with monotonic activation function $\sigma$, $\overline z_k = h(\overline z_{k-1})$ and $\underline z_k = h(\underline z_{k-1})$ .</p></li><li><p>Let $\overline z_0(\epsilon) = z_0 + \epsilon \mathbf 1$ and $\underline z_0(\epsilon) = z_0 - \epsilon \mathbf 1$.</p></li><li><p>Left hand size of $\ref{verify}$ is bounded by $\overline z_{k,y}(\epsilon) - \underline z_{k,true}(\epsilon)$. To minimuze this term, define:
$$
z^*_{k,y}(\epsilon) = \begin{cases}\overline z_{k,y}(\epsilon)&\text{if } y\neq y_{true}\ \underline z_{k,y}(\epsilon)&\text{if }y = y_{true}\end{cases}
$$</p></li><li><p>Then minimize hybrid training loss:
$$
\mathcal L = \ell(z_k,y_{true}) + \alpha \ell(z^*_{k}(\epsilon), y_{true})
$$</p></li></ul><h3 id=8-physical-world-attacks>8 Physical World Attacks</h3><p><em><strong>Synthesizing Robust Adversarial Examples</strong></em></p><p><strong>Expectation Over Transformation</strong></p><ul><li><p>To address the issue: adversarial examples does not keep adversarial under image transformations in the real world.</p></li><li><p>Minimize visual difference $t(x)-t(x^\prime)$ instead of $x-x^\prime$ in texture space</p></li></ul><p>$$
\begin{align}
\arg\max_{x^\prime} \quad&\mathbb E_{t\sim T}[\log P(y_t|t(x^\prime))]\<br>\mathrm{s.t.} \qquad&\mathbb E_{t\sim T} [d(t(x^\prime), t(x))]&lt;\epsilon\notag\<br>&x^\prime \in [0,1]^d\notag
\end{align}
$$</p><ul><li><p>The distribution $T$ of transformations:</p><ul><li><p>2D: $t(x) = Ax + b$</p></li><li><p>3D: texture $x$, render it on an object to $Mx +b$</p></li></ul></li><li><p>Optimize the objective:
$$
\arg\max_{x^\prime} \ \mathbb E_{t\sim T}\big[\log P(y_t|t(x^\prime)) - \lambda |LAB(t(x)) - LAB(t(x^\prime))|_2\big]
$$</p></li></ul><p><em><strong>Fooling Automated Surveillance Cameras Adversarial Patches to Attack Person Detection</strong></em></p><ul><li><p>Patch Adversarial Attack: only structurally editing certain local areas on an image</p></li><li><p>A pipeline of <strong>patch attack</strong></p><p><img src="figure/Adv_Patch_Pipeline.png" alt=avatar style=zoom:45%></p></li><li><p>Hybrid Objectives:</p><ul><li><p>$L_{nps}$ non-printability score</p></li><li><p>$L_{tv}$ the total variation loss. Force the image to be smooth.
$$
L_{tv} = \sum_{i,j} \sqrt{(p_{i,j} - p_{i+1,j})^2 + (p_{i,j} - p_{i,j+1})^2}
$$</p></li><li><p>$L_{obj}$ maximize the objectness $p(obj)$. Note that we can also use $L_{cls}$ (class score) or both.</p></li></ul></li></ul><p><em><strong>Adversarial T-shirt! Evading Person Detectors in A Physical World</strong></em></p><h5 id=thin-plate-spline-tps-mapping>Thin Plate Spline (TPS) mapping</h5><ul><li><p>To learn transformations $t$ that maps each pixel $p^{(x)}$ to $p^{(z)}$.</p></li><li><p>Suppose $p^{(x)} = (\phi^{(x)}, \psi^{(x)})$, $p^{(z)} = (\phi^{(x)}+\Delta_\phi, \psi^{(x)}+\Delta_\psi)$.</p></li><li><p>According to TPS method, the only solution of $\Delta$ is given by:
$$
\Delta(p^{(x)};\theta) = a_0 +a_1\phi^{(x)} + a_2 \psi^{(x)} + \sum_{i=1}^n c_i U(|\hat p_i^{(x)} - p^{(x)}|_2) \label{delta}
$$
where the radial basis function $U(r) = r^2 \log r$ and $\hat p_i^{(x)}$ are $n$ sampled points on image $x$.</p></li><li><p>TPS resorts to a regression problem to determine $\theta$, in which the regression objective is to minimize the difference between
$$
{\Delta(\hat p_i^{(x)};\theta)}<em>{i=1}^n \quad \text{and} \quad {(\phi_i^{(z)}, \psi_i^{(z)}) - (\phi_i^{(x)},\psi_i^{(x)})}</em>{i=1}^n
$$</p></li><li><p>This results in an equivalent problem:
$$
F\theta_\phi =\begin{pmatrix}K&P\P^T &0_{3\times 3}
\end{pmatrix}\theta_\phi = \begin{pmatrix}\hat \Delta_\phi\ 0_{3\times 1}\end{pmatrix}^T
$$
where $K_{ij} = U(|\hat p_{i}^{(x)} - \hat p_j^{(x)}|)$ $\theta_\phi = [c,a]$ and $P = [1, \hat \phi^{(x)}, \hat\psi^{(x)}]$.</p><p>(See <a href="index.html#2-tps_grid_gen.py-%28TPS%29">Code for TPS</a> for implementing details.)</p></li></ul><h5 id=adversarial-t-shirts-generation>Adversarial T-shirts generation</h5><ul><li>The pipeline is similar as above. The major difference is the composited transformation adopted here.</li><li>The overall transformation is given by:</li></ul><p>$$
x_i^\prime = t_{env}(A + t(B - C+t_{color}(M_{c,i}\circ t_{TPS}(\delta + \mu v)))), t\sim \mathcal T, t_{TPS}\sim \mathcal T_{TPS}, v\sim \mathcal N(0,1)
$$</p><ul><li>$A = (1-M_{p,i})\circ x_i$ yields the background region, $B = M_{p,i}\circ x_i$ is the human-bounded region.</li><li>$C = M_{c,i}\circ x_i$ is the bounding box of T-shirt.</li><li>$t_{color}$ is applied in place of non-printability loss.</li><li>$t$ stands for conventional physical transformations, $t_{env}$ for brightness of the whole environment.</li><li>Gaussian smoothing is applied by $v$ to the adversarial patch.</li></ul><p><em><strong>Can 3D Adversarial Logos Cloak Humans?</strong></em></p><ul><li><p>Various postures and multi-view transformations threatens the adversarial property of previous 2D adversarial patches</p></li><li><p>Overall pipeline: Detach 3D logos from person mesh as submeshes $\mathcal L$, then:
$$
\tilde{\mathcal L} = \mathcal T_{logo}(S,\mathcal L) = \mathcal M_{3D}(\mathcal S, \mathcal M_{2D}(\mathcal L))
$$</p><ul><li><p>Texture $\mathcal S$</p></li><li><p>$\mathcal M_{2D}$ maps a 3D logo to 2D domain $[0,1]^2$; $M_{3D}$ attach texture to 3D logo</p></li></ul><p>Finally, render the 3D adv logo by differentiable renderer (e.g. Neural 3D Mesh Renderer) with human and background.</p></li><li><p><strong>Loss</strong></p></li></ul><p>$$
\mathcal L_{adv} = \lambda \cdot DIS(\mathcal I, y) + TV(\tilde{\mathcal L})
$$</p><ul><li>DIS: disappearance loss = the maximum confidence of all bounding boxes that contain the target object</li><li>TV: total variance: $TV(\tilde{\mathcal L}) = \sum_{i,j} (|R(\tilde{\mathcal L})_{i,j}- R(\tilde{\mathcal L})_{i,j+1}| + |R(\tilde{\mathcal L})_{i+1,j}- R(\tilde{\mathcal L})_{i,j}|)$ captures discontinuity of 2D adv logo. (Here $R$ stands for rendering.)</li></ul><p><em><strong>Adversarial Texture for Fooling Person Detectors in Physical World</strong></em></p><ul><li><blockquote><p>Goal: to train an expandable texture that can cover any clothes in any size</p></blockquote></li><li><p>Four methods: RCA, TCA, EGA, TC-EGA</p></li><li><p><a href="index.html#Code:-Adversarial-Texture">Code Notes</a></p></li></ul><h3 id=9-object-detection>9 Object Detection</h3><h5 id=91-yolo>9.1 YOLO</h5><ul><li><p>$S\times S$ grids, each containing $B$ anchor points with bounding boxes</p></li><li><p>Each anchor point: $[x,y,w,h,p_{obj}, p_{\ell1}, \dots, p_{\ell n}]$</p></li><li><p>$p_{obj}$: object probability. The prob. of containing an object.</p></li><li><p>$p_{\ell i}$: Class score, learned by SoftMax and cross entropy</p></li><li><p>Confidence of object: measured by $p_{obj} \times IOU$.</p></li><li><p>Confidence of class: measured by $p_{obj}\times IOU \times \Pr[\ell_i,|,obj]$</p></li><li><p>Yolo: Outputs [<code>batch</code>, <code>num_class</code> + 5$\times$<code>num_anchors</code> , $H\times W$]</p></li><li><p>Yolov2: Outputs [<code>batch</code>, (<code>num_class</code> + 5)$\times$<code>num_anchors</code> , $H\times W$] (See details at <a href="index.html#3.1-MaxProbExtractor">below</a>).</p></li></ul><h5 id=92-region-proposal-network>9.2 Region proposal network</h5><ul><li>CNN generates anchors:<ul><li>For each pixel on the feature map (say 256 dimension with size H$\times W$), generate $k=9$ anchors.</li><li>The height-weight ratio of these 9 anchors are 0.5, 1 or 2, each with three different size.</li><li>Each pixel has $2k$ scores and $4k$ coordinates. Each anchor yields a foreground and a background score. Use softmax to decide where it is foreground or background.</li></ul></li><li>Meanwhile, use <strong>bounding box regression</strong> on each anchor. (Another branch)</li><li>Finally, <strong>Proposal Layer</strong> takes sum over anchors and BBox regression.<ul><li>Sort these anchors by foreground softmax scores.</li><li>Delete anchors that surpass too much from boundary.</li><li>Use <strong>Non-maximum suppression</strong> to avoid multiple anchors on a single object. (Recursively choose the anchor with highest score and delete other anchors with high IOU against it.)</li></ul></li></ul><h5 id=93-bounding-box>9.3 Bounding Box</h5><ul><li><p>Original bounding box $P(x,y,w,h)$, learn deformation $d(P)$ to approximate the ground truth
$$
\hat G_x = P_w d_x(P)+P_x\<br>\hat G_y = P_h d_y(P)+P_y\<br>\hat G_w = P_w e^{d_w(P)}\<br>\hat G_h = P_h e^{d_h(P)}\<br>$$</p></li><li><p>where $d(P) = w^T\phi(P)$. $\phi$ is the feature vector so we shall learn parameter $w$</p></li></ul><h5 id=94-roi-alignment>9.4 ROI Alignment</h5><ul><li>The proposed anchors have different size $(w,h)$, pool the corresponding feature map (with size $w/16,h/16$) to a fixed size $(w_p, h_p)$. In each of these $w_ph_p$ grids, do max pooling.</li><li>Finally, apply FC layers to calculate class probability and use bounding box regression again.</li></ul><h3 id=10-basic-graphics>10 Basic Graphics</h3><h5 id=101-coordinates>10.1 Coordinates</h5><ul><li><p>World coordinates: $(x,y,z)$ means left, up and in.</p><ul><li>Azimuth: 经度角</li></ul></li><li><p>Camera Projection Matrix $K$ (intrinsic parameters of a camera)
$$
\lambda \begin{pmatrix}u\v\1\end{pmatrix} = \begin{pmatrix}f&&p_x\&f&p_y\&&1\end{pmatrix}\begin{pmatrix}X\Y\Z\end{pmatrix} = K\mathbf X_c
$$</p><ul><li>From 3D world (metric space) to 2D image (pixel space)</li></ul></li><li><p>Coordinate transformation from <strong>world coordinate</strong> $\mathbf X$ to <strong>camera coordinate</strong> $\mathbf X_c$:
$$
\mathbf X_c = R\mathbf X + t = \begin{pmatrix}\mathbf R_{3\times 3} &\mathbf t_{3\times 1}\end{pmatrix}\begin{pmatrix}\mathbf X\1\end{pmatrix}
$$</p></li></ul><h5 id=102-obj-format>10.2 Obj format</h5><ul><li>vertex: 3D coordinate. In format: <code>v x y z</code></li><li>vertex texture: 2D coordinate in texture figure. In format: <code>vt x y</code></li><li>vertex normal: normal direction. In format: <code>vn x y z</code></li><li>face. In format: <code>f v1/vt1/vn1 v2/vt2/vn2 v3/vt3/vn3</code>.</li><li>See examples <a href=https://dl.fbaipublicfiles.com/pytorch3d/data/cow_mesh/cow.obj target=_blank rel=noopener>here</a></li></ul><h5 id=103-pytorch3d>10.3 Pytorch3d</h5><ul><li><p>load an object</p><ul><li><code>verts, faces, aux = load_obj(obj_dir)</code></li><li>OR <code>mesh = load_objs_as_meshes([obj_dir], device)</code></li></ul></li><li><p>Mesh: Representations of vertices and faces</p><ul><li><p>​ List | Padded | Packed</p></li><li><p>$[[v_1],\dots, [v_n]]$ | has batch dimension | no batch dimension, index into padded representatoin</p></li><li><p>e.g. <code>vertex = mesh.verts_packed()</code></p></li></ul></li><li><p>Mesh.textures:</p><ul><li><p>Three possible representations:</p><ul><li><p>TexturesAtlas (each face has a texture map)</p><ul><li>(N,F,R,R,C): each face use $R\times R$ grid</li></ul></li><li><p>TexturesUV: a UV map from vertices to texture image</p></li><li><p>TexturesVertex: a color for each vertex</p></li></ul></li></ul><pre><code class=language-python>#for uv:
mesh.textures.verts_uvs_padded()
#for TexturesVertex:
rgb_texture = torch.tensor([1,vertex.shape[0], 3]).uniform_(0,1)
mesh.textures = TexturesVertex(vertex_features = rgb_texture)
</code></pre></li></ul><h5 id=104-render>10.4 Render</h5><ul><li>Luminous Flux: $dF = dE/(dS\cdot dt)$.</li><li>Radiance: $I = dF/d\omega$. (立体角)</li><li>Conservation:<ul><li>$I_i = I_d + I_s + I_t +I_v$.</li><li>Diffuse light: $I_d = I_i K_d (\vec L\cdot\vec N)$<ul><li>where $\vec L$ is the orientation of the initial light and $\vec N$ is the normal orientation.</li></ul></li><li>Specular light: $I_s = I_i K_s(\vec R \cdot \vec V)^n$<ul><li>where $\vec R$ is the reflective light and $\vec V$ is the direction of view.</li></ul></li><li>Ambient light: $I_a = I_i K_a$.</li></ul></li><li>Shading<ul><li>Gouraud: Color interpolation (barycentric interpolation)</li><li>Phong: Normal vector interpolation</li></ul></li></ul><h3 id=11-others>11 Others</h3><h5 id=111-entropy-kl-divergence>11.1 Entropy, KL divergence</h5><ul><li><p>Entropy $H(X) = -\sum_{x\in X}p(x)\log p(x)$.</p></li><li><p>Cross entropy $XE(p,q) = \mathbb E_p (-\log q)$.</p></li><li><p>The distance between two distributions $p$ and $q$ can be measured by:
$$
KL(p|q) = \sum_{x\in X}p(x)\log \frac{p(x)}{q(x)} = XE(p,q) - H(p),
$$
which represents the information loss of describing $p(x)$ by $q(x)$.</p></li><li><p>Mutual Information: $\mathbb I(X;Y) = KL(p(X,Y)|p(X)p(Y))$.</p></li></ul><h5 id=112-statistics>11.2 Statistics</h5><ul><li>Accuracy = $\frac{TP+TN}{TP+TN+FP+FN}$</li><li>Precision = $\frac{TP}{TP+FP}$</li><li>Recall = $\frac{TP}{TP+FN}$</li><li>PR-curve: traverses all outoffs to get a tradeoff curve of precision and recall</li></ul><h3 id=12-experiments>12 Experiments</h3><ul><li><p>FGSM, BIM, Carlini & Wagner attacks</p></li><li><p>Adversarial Training</p><ul><li>FGSM adversarial training</li></ul></li></ul><p><img src="https://chuwd19.github.io/home/qianhw/%E4%B8%8B%E8%BD%BD/hugo/content/slides/notes/figure/FGSM_adv_train.png" alt=avatar style=zoom:36%></p><table><thead><tr><th></th><th>Accuracy</th><th>FGSM ($ e=4/255$)</th><th>CW ($ e = 4/255,a= 0.01, K = 10$)</th></tr></thead><tbody><tr><td>75999.pth</td><td>0.817</td><td>0.6634</td><td>0.099</td></tr></tbody></table><ul><li>Adversarial Texture:<ul><li>TCA-1000epoch: AP = 0.6395</li><li>TCEGA-2000,1000: AP = 0.4472</li><li>TCEGA-HSV-red-2000,1000: AP = 0.6951</li><li>TCEGA-Gaussian-2000,1000: AP = 0.4916</li></ul></li></ul><h5 id=pytorch3d-experiments>Pytorch3d Experiments</h5><h5 id=adv_3d>Adv_3d</h5><ul><li>Differentiable Rendering + original adv_patch pipeline</li><li>MaxProbExtractor: Only optimize the box with max iou!</li></ul><p>Issues:</p><ul><li>Parrallel<ul><li>solved by modifying detection/transfer.py</li><li>may introduce problems of space redundancy</li><li>Config now: batch size = 2, num_views = 4, any bigger batch size causes cuda out of memory</li><li>10 minutes/batch</li></ul></li><li>Project a [3,H,W] cloth to TextureAtlas<ul><li>try TextureUV, but the projection from texture.jpg to TextureUV seems not differentiable</li></ul></li><li>Add more constraints?</li><li>Ensemble learning</li></ul><p>Experiment1: Batch: $2\times 4$, lr = 0.001, attack faster-rcnn</p><p><img src="figure/experiment1/epoch.png" alt=epoch style=zoom:25%></p><p><img src="figure/experiment1/patch.jpg" alt=patch style=zoom:50%></p><p><img src="figure/experiment1/test0.png" alt=avatar style=zoom:33%></p><ul><li><p>The tendency of attacking two-stage detectors such as faster-rcnn: split boxes to smaller ones</p></li><li><p>MaxProbExtractor: Only to attack the box with max iou may sacrifice those boxes with smaller iou but much higher probability? (<strong>Failed</strong>, the current method works great enough)</p><ul><li>now: iou threshold 0.4, prevent over-optimizing on trivial boxes.</li><li>try attacking the box with max confidence = iou $\times$ prob?</li></ul></li><li><p>We now take the mean of gradient over $B$ pictures. Why not try weighted mean (e.g. $\ell_2$) or other loss functions (e.g. $\sum e^{prob}$) to urge the trainer to attack the largest max_prob boxes?</p></li><li><p>Model placed in the middle of the picture (Overfit?) (Usually <strong>not</strong> a problem here)</p></li><li><p>8.28: I observe that over the parameters in the shape of [1,6906,8,8,3], only 3.49% of them (46333) deviate from original setup 0.5 (for grey). Over the trained parameters, 18.7% of them go beyond the [0,1] range.</p></li><li><p>8.31 I render the patch trained by 4 viewing points (0,90,180,270), it turns out that a small deviation from these angles would make the rendered picture almost completely grey:</p><ul><li>It turns out that this is due to the Atlas expression of texture</li></ul><p><img src="figure/Tshirt_azim.png" alt=avatar style=zoom:20%></p></li><li><p>8.31 I try 50% droppout on the adv patch (a random 0/1 mask of size 6000):</p><ul><li>100%： recall = 0.10, 80%: recall = 0.32, 50%: recall = 0.89. <strong>(fail)</strong></li></ul></li><li><p>9.1 experiment4: random angles (163937) <strong>(fail)</strong></p><ul><li><p>parameters 87.59% trained</p></li><li><p>没有形成完整连续的图像，几乎没有对抗效果 (recall = 0.96)，但loss一直在0.3上下</p></li><li><p>I fixed the viewing angles for each epoch, so perhaps the tshirt is trained only adversarial for those views at end of each epoch. (<strong>fixed later in experiment 7</strong>)</p></li></ul><p><img src="https://chuwd19.github.io/home/qianhw/%E4%B8%8B%E8%BD%BD/hugo/content/slides/notes/figure/Tshirt_random_angle.png" alt=avatar style=zoom:20%></p></li><li><p>9.4 experiment5: vec2atlas, R = 8. (Map $(3,V)$ to atlas $(1,V,R,R,3)$ before the previous pipeline).</p><ul><li>recall = 0.20</li></ul></li><li><p>9.3 experiment6: vec2atlas, R=2.</p><ul><li>Reducing parameter $R$ does not influence the quality of the rendered pics much, but save memory and time.</li></ul><p><img src="figure/Tshirt_vec_R=2_random.png" alt=avatar style=zoom:28%></p></li><li><p>it seems that R=8 introduces too much parameters for a normal tshirt</p></li><li><p>experiment7: R=2, random angle, switch every 20 iterations, vec2atlas</p><p><img src="figure/experiment&#32;7/loss-curve.png" alt=avatar style=zoom:28%></p><center style=color:#C0C0C0;text-decoration:underline>Loss curve for random angle sampling</center><ul><li>It turns out that random sampling takes about three times the epoches to converge as using fixed angles, but the figure below demonstrates the failure of the latter option on universal angles.</li></ul></li></ul><p><img src="figure/recall-angle-compare.png" alt=avatar style=zoom:36%></p><center style=color:#C0C0C0;text-decoration:underline>conf_thresh = 0.01, iou_thresh = 0.5</center><ul><li><p>9.5 experiment 8： 尝试不均匀地sample角度，因为之前 random angles 均匀采样（as the red line shows）会导致面积较小的衣服侧面对抗性较低</p><ul><li>evaluate the model once every 5 epoches, divide the $360^\circ$ angles into 36 intervals and estimate the loss $\ell_i$ in each interval.</li><li>Sample $azim \leftarrow D$, where $D(i) = \exp (\alpha\ell_i) / \sum_i \exp (\alpha\ell_i) $</li></ul></li><li><p>9.7 I test the performance of different $\alpha$. Since the final loss ranges from 0.1 to 0.25, I try $\alpha = 10, 15, 20$ so that the ratio of sampling probability is about $\sim 10$.</p><ul><li>$\alpha = 10$ is too weak to be efficient; while $\alpha = 20$ is too aggressive to converge.</li><li>$\alpha = 15$ is balancing.</li></ul><p><img src="figure/recall-angle-exp-sampling.png" alt=avatar style=zoom:36%></p></li><li><p>9.9 I regenerate an obj file for Tshirt using meshlab.</p><ul><li>Details: Set up 4 cameras (at 0,90,180,270 degree) and auto-generate the maps from mesh to texture.</li></ul><p><img src="figure/textureuv.png" alt=avatar style=zoom:20%></p></li><li><p>9.10 Map the $(3,V)$ vector to the uv texture.</p><ul><li><p>Details: Draw a monochrome triangle on the texture for each face according to $(3,V)$</p></li><li><p>The expressive power of uv texture is much stronger than $(3,V)$. The reverse mapping thus requires more restriction.</p></li><li><p>Render from the texture again using the UV map.</p></li></ul><p><img src="figure/atlas2uv/texture.png" alt=avatar style=zoom:48%></p><p><img src="figure/atlas2uv/Tshirt_render-compare.png" alt=avatar style=zoom:28%></p><ul><li><p><del>The uv-rendered tshirt is smoother in color but much less adversarial than the atlas-rendered one.</del></p></li><li><p><del>It is necessary to create a precise mapping from UV to Atlas, which would enable the pipeline of training an adversarial uv texture.</del></p><p><del><img src="https://chuwd19.github.io/home/qianhw/%E4%B8%8B%E8%BD%BD/hugo/content/slides/notes/xfigure/atlas2uv/recall-angle-compare.png" alt=avatar style=zoom:36%></del></p></li><li><p><del>An observation is that the lateral part of the uv-rendered tshirt gives lower recall, which is counterintuitive since the lateral part usually performs worse than other angles with less surface area.</del></p></li><li><p><del>A possible (yet not necessarily true) explanation: the task of the lateral parts is harder so it is trained more robust to random deviations.</del></p></li><li><p><del>(9.12) Combining two meshes using uv texture causes conflicts: mesh of man cloaks the mesh of tshirt</del></p><p><del><img src="https://chuwd19.github.io/home/qianhw/%E4%B8%8B%E8%BD%BD/hugo/content/slides/notes/afigure/atlas2uv/issue.png" alt=avatar style=zoom:33%></del></p></li><li><p>This bug is due to incompatible texture size of two meshes. <strong>Fixed</strong>. (9.16)</p></li></ul></li><li><p>Transfer uv texture back to $(3,V)$ by interpolation (3% deviation from original $(3,V)$ representation).</p></li><li><p>9.15 Enables the fast transfer from (3,V) to 2d texture in pipeline and calculate the corresponding TV loss of the 2d texture. <code>loss = det_loss + a * tv_loss</code></p><p><img src="figure/texture_tv.png" alt=avatar style=zoom:8%></p><ul><li>Details: <code>uv = vec[:,maps[:,:]]</code></li></ul></li></ul><p><img src="https://chuwd19.github.io/home/qianhw/%E6%A1%8C%E9%9D%A2/Adversarial%20ML/figure/recall-angle-compare-tvloss.png" alt=avatar style=zoom:33%></p><ul><li><p>Current Pipeline:</p><p><img src="https://chuwd19.github.io/home/qianhw/%E4%B8%8B%E8%BD%BD/hugo/content/slides/notes/figure/pipeline0.png" alt=avatar style=zoom:33%></p></li><li><p>Next step: to enable the rendering process directly from TextureUV.</p><ul><li>Replaces TextureAtlas and (3,V) with TextureUV</li><li>Facilitates direct modification on Tshirt cloth</li></ul></li><li><p>9.16 Merge multiple pieces of texture maps into one.</p><ul><li>Details: Regenerate an obj. for man with nonoverlapping texture map.</li><li>Load the origin obj. file using atlas and transform it into (3,V) form.</li><li>Read the new obj. file by hand and draw each faces using PIL.draw.</li></ul></li></ul><p><img src="figure/texture_man.png" alt=avatar style=zoom:33%></p><p>Pipeline:</p><p><img src="figure/pipeline1.png" alt=avatar style=zoom:33%></p><p>Results:</p><p><img src="figure/experiment14.png" alt=avatar style=zoom:33%></p><p><img src="figure/experiment14_render.png" alt=avatar style=zoom:50%></p><p><img src="figure/pipeline2.png" alt=avatar style=zoom:33%></p><ul><li>Collect data of fashionable T-shirts (about 1300 tshirt clean images)</li><li>Use WGAN to generates TextureUV similar to normal T-shirts</li><li>$z\in \mathbb R^{128}$, sampled from $\mathcal N(0,I)$.</li><li>May require training of $z$.</li></ul><center class=half><img src="figure/generate/gen1.png" alt=avatar style="margin:0 10px" width=400>
<img src="figure/generate/gen3.png" alt=avatar style="margin:0 10px" width=400><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#123;padding:1px">left: WGAN, Loss = det loss + 0.04*LossG;</div><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#123;padding:1px">right: Loss = det loss</div></center><p>​</p><ul><li>Problems: GAN 不稳定, 且 generator 学不到数据中的style<ul><li>数据集style更集中</li><li>VAE reconstruction，then train latent vector for adversarial loss</li></ul></li></ul><h3 id=code-adversarial-texture>Code: Adversarial Texture</h3><h5 id=1-training_texturepy-main>1 training_texture.py (Main)</h5><ul><li>adversarial cloth: <code>[1(batch),3(RGB),width, height]</code></li><li>Random Crop Attack (RCA), Toroidal Crop Attack (TCA) differs only at <code>random_crop</code></li></ul><h5 id=2-tps_grid_genpy-tps>2 tps_grid_gen.py (TPS)</h5><ul><li><p>Initialize: Using a $N\times 2$ array, denoting the $N$ target control points. Then construct the TPS kernel matrix as shown <a href="index.html#Thin-Plate-Spline-%28TPS%29-mapping">above</a>. <code>target_control_points</code>: $\hat p_i^{(x)}, i =[1,\dots, 25]$.</p></li><li><p><code>source_control_point</code> is sampled with small disturb from <code>target_control_points</code>, which stands for $\hat p_i^{(z)}$.</p></li><li><p><code>source_coordinate = self.forward(source_control_points)</code>.</p><ul><li><p>forward function calculates
$$
F^{-1}\begin{pmatrix}\hat\Delta_{(\phi,\psi)}\0_{3\times 2}\end{pmatrix}^T = [\theta_\phi,\theta_\psi]
$$</p></li><li><p>Then calculate <code>source_coordinate</code> by equation $\ref{delta}$.</p></li></ul></li></ul><pre><code class=language-python>mapping_matrix = torch.matmul(Variable(self.inverse_kernel), Y) 
source_coordinate = torch.matmul(Variable(self.target_coordinate_repr), mapping_matrix)
</code></pre><ul><li>Finally, use <code>F.grid_sample</code> to map the adversarial patch to <code>source_coordinate</code>.</li></ul><h5 id=3-load_datapy>3 load_data.py</h5><h6 id=31-maxprobextractor>3.1 MaxProbExtractor</h6><ul><li><p>Extracts max class probability from YOLO output.</p></li><li><p>YOLOv2 output: [<code>batch</code>, (<code>num_class</code> + 5)$\times$<code>num_anchors</code> , $H\times W$]</p></li><li><p><code>num_class</code> + 5 = 85.</p><ul><li><p>0~3: x,y,w,h</p></li><li><p>4: confidence of this anchor (objectness)</p></li><li><p>5~84: class probability $\Pr[class_i|obj]$ of this anchor</p></li><li><p>for <code>func = lambda obj,cls:obj</code>, we only minimize the maximum objectness confidence.</p></li></ul></li></ul><h5 id=4-random_crop>4 random_crop</h5><p>​ Crop type:</p><ul><li>None: used for RCA, TCA crop</li></ul><h5 id=5-patch-transformer>5 Patch transformer</h5><ul><li>randomly adjusting brightness and contrast, adding random amount of noise, and rotating randomly</li><li><code>adv_batch = adv_batch * contrast + brightness + noise</code></li><li>The training label: (N, num_objects, 5).</li><li>Output: (N, num_objects, 3, fig_h, fig_w)</li></ul><h3 id=paper-list>Paper List</h3><p>Most parts of this paper list is borrowed from <a href=https://nicholas.carlini.com/writing/2018/adversarial-machine-learning-reading-list.html target=_blank rel=noopener>Nicholas Carlini&rsquo;s Reading List</a>.</p><h5 id=preliminary-papers>Preliminary Papers</h5><p>==<a href=https://arxiv.org/abs/1708.06131 target=_blank rel=noopener>Evasion Attacks against Machine Learning at Test Time</a>==
==<a href=https://arxiv.org/abs/1312.6199 target=_blank rel=noopener>Intriguing properties of neural networks</a>==
==<a href=https://arxiv.org/abs/1412.6572 target=_blank rel=noopener>Explaining and Harnessing Adversarial Examples</a>==</p><h5 id=attacks-requires-preliminary-papers>Attacks [requires Preliminary Papers]</h5><p>==<a href=https://arxiv.org/abs/1511.07528 target=_blank rel=noopener>The Limitations of Deep Learning in Adversarial Settings</a>==
<a href=https://arxiv.org/abs/1511.04599 target=_blank rel=noopener>DeepFool: a simple and accurate method to fool deep neural networks</a>
==<a href=https://arxiv.org/abs/1608.04644 target=_blank rel=noopener>Towards Evaluating the Robustness of Neural Networks</a>==</p><h5 id=transferability-requires-preliminary-papers>Transferability [requires Preliminary Papers]</h5><p>==<a href=https://arxiv.org/abs/1605.07277 target=_blank rel=noopener>Transferability in Machine Learning: from Phenomena to Black-Box Attacks using Adversarial Samples</a>==
<a href=https://arxiv.org/abs/1611.02770 target=_blank rel=noopener>Delving into Transferable Adversarial Examples and Black-box Attacks</a>
==<a href=https://arxiv.org/abs/1610.08401 target=_blank rel=noopener>Universal adversarial perturbations</a>==</p><h5 id=detecting-adversarial-examples-requires-attacks-transferability>Detecting Adversarial Examples [requires Attacks, Transferability]</h5><p>==<a href=https://arxiv.org/abs/1702.04267 target=_blank rel=noopener>On Detecting Adversarial Perturbations</a>
<a href=https://arxiv.org/abs/1703.00410 target=_blank rel=noopener>Detecting Adversarial Samples from Artifacts</a>
<a href=https://arxiv.org/abs/1705.07263 target=_blank rel=noopener>Adversarial Examples Are Not Easily Detected: Bypassing Ten Detection Methods</a>==</p><h5 id=restricted-threat-model-attacks-requires-attacks>Restricted Threat Model Attacks [requires Attacks]</h5><p><a href=https://arxiv.org/abs/1708.03999 target=_blank rel=noopener>ZOO: Zeroth Order Optimization based Black-box Attacks to Deep Neural Networks without Training Substitute Models</a>
==<a href=https://arxiv.org/abs/1712.04248 target=_blank rel=noopener>Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models</a>==
<a href=https://arxiv.org/abs/1807.07978 target=_blank rel=noopener>Prior Convictions: Black-Box Adversarial Attacks with Bandits and Priors</a></p><h5 id=verification-requires-introduction>Verification [requires Introduction]</h5><p><a href=https://arxiv.org/abs/1702.01135 target=_blank rel=noopener>Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks</a>
<a href=https://arxiv.org/abs/1810.12715 target=_blank rel=noopener>On the Effectiveness of Interval Bound Propagation for Training Verifiably Robust Models</a></p><h5 id=defenses-2-requires-detecting>Defenses (2) [requires Detecting]</h5><p><a href=https://arxiv.org/abs/1706.06083 target=_blank rel=noopener>Towards Deep Learning Models Resistant to Adversarial Attacks</a>
==<a href=https://arxiv.org/abs/1802.03471 target=_blank rel=noopener>Certified Robustness to Adversarial Examples with Differential Privacy</a>==</p><h5 id=attacks-2-requires-defenses-2>Attacks (2) [requires Defenses (2)]</h5><p>==<a href=https://arxiv.org/abs/1802.00420 target=_blank rel=noopener>Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples</a>==
<a href=https://arxiv.org/abs/1802.05666 target=_blank rel=noopener>Adversarial Risk and the Dangers of Evaluating Against Weak Attacks</a></p><h5 id=defenses-3-requires-attacks-2>Defenses (3) [requires Attacks (2)]</h5><p><a href=https://arxiv.org/abs/1805.09190 target=_blank rel=noopener>Towards the first adversarially robust neural network model on MNIST</a>
<a href=https://arxiv.org/abs/1902.06705 target=_blank rel=noopener>On Evaluating Adversarial Robustness</a></p><h5 id=other-domains-requires-attacks>Other Domains [requires Attacks]</h5><p><a href=https://arxiv.org/abs/1702.02284 target=_blank rel=noopener>Adversarial Attacks on Neural Network Policies</a>
<a href=https://arxiv.org/abs/1801.01944 target=_blank rel=noopener>Audio Adversarial Examples: Targeted Attacks on Speech-to-Text</a>
<a href=https://arxiv.org/abs/1803.01128 target=_blank rel=noopener>Seq2Sick: Evaluating the Robustness of Sequence-to-Sequence Models with Adversarial Examples</a>
<a href=https://arxiv.org/abs/1702.06832 target=_blank rel=noopener>Adversarial examples for generative models</a></p><h5 id=detection>Detection</h5><p><a href=https://arxiv.org/abs/1311.2524 target=_blank rel=noopener>Rich feature hierarchies for accurate object detection and semantic segmentation</a>
==<a href=https://arxiv.org/abs/1506.02640 target=_blank rel=noopener>You Only Look Once: Unified, Real-Time Object Detection</a>==
==<a href=https://arxiv.org/abs/1612.08242 target=_blank rel=noopener>YOLO9000: Better, Faster, Stronger</a>==</p><h5 id=physical-world-attacks>Physical-World Attacks</h5><p>==<a href=https://arxiv.org/abs/1607.02533 target=_blank rel=noopener>Adversarial examples in the physical world</a>==
==<a href=https://arxiv.org/abs/1707.07397 target=_blank rel=noopener>Synthesizing Robust Adversarial Examples</a>==
<a href=https://arxiv.org/abs/1707.08945 target=_blank rel=noopener>Robust Physical-World Attacks on Deep Learning Models</a>
==<a href=https://arxiv.org/abs/1910.11099 target=_blank rel=noopener>Adversarial T-shirt! Evading Person Detectors in A Physical World</a>==
<a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Huang_Universal_Physical_Camouflage_Attacks_on_Object_Detectors_CVPR_2020_paper.pdf#:~:text=UPC%20constructs%20a%20universal%20camou%EF%AC%82age%20pattern%20for%20ef-fectively,patterns%20on%20object%20surfaces%20such%20as%20humanaccessories%2Fcar%20paintings." target=_blank rel=noopener>Universal Physical Camouflage Attacks on Object Detectors</a>
==<a href=https://openaccess.thecvf.com/content_CVPRW_2019/papers/CV-COPS/Thys_Fooling_Automated_Surveillance_Cameras_Adversarial_Patches_to_Attack_Person_Detection_CVPRW_2019_paper.pdf target=_blank rel=noopener>Fooling Automated Surveillance Cameras Adversarial Patches to Attack Person Detection</a>==
==<a href=https://arxiv.org/abs/2006.14655 target=_blank rel=noopener>Can 3D Adversarial Logos Cloak Humans?</a>==
==Adversarial Texture for Fooling Person Detectors in Physical World==</p><h3 id=ideas>Ideas</h3><ul><li>Difference from 3D logo? (What&rsquo;s our goal?)</li><li>Restricted deformation or recoloring from any input cloth?</li><li>Differential deformation of logo (by B-spline?)</li><li>monochromatic, analogous, or complementary colors</li></ul><p>我们现在是优先attackiou最大的框，然后小于一定iou threshold的就不训练了，防止过度训练到一些trivial的boxes</p><p>牺牲了一些iou比较小但是prob比较大的框，能不能把周围有人的情况下，把周围的人也隐藏起来</p><p>object confidence=iou和prob 效果不好</p><p>B个角度的取梯度的平均值，weighted mean去加速优先attack</p><p>2D的pipeline 饱和度 hsv</p><p>色相饱和度亮度</p><p>参数化 gan</p></div><div class=article-tags><a class="badge badge-light" href="../../tag/typed/index.html">typed</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://chuwd19.github.io/archived_note/adversarial-ml/&text=Adversarial%20Machine%20Learning" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://chuwd19.github.io/archived_note/adversarial-ml/&t=Adversarial%20Machine%20Learning" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Adversarial%20Machine%20Learning&body=https://chuwd19.github.io/archived_note/adversarial-ml/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://chuwd19.github.io/archived_note/adversarial-ml/&title=Adversarial%20Machine%20Learning" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Adversarial%20Machine%20Learning%20https://chuwd19.github.io/archived_note/adversarial-ml/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://chuwd19.github.io/archived_note/adversarial-ml/&title=Adversarial%20Machine%20Learning" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href="../../index.html"><img class="avatar mr-3 avatar-circle" src="../../authors/admin/avatar.jpg" alt="Wenda Chu 储闻达"></a><div class=media-body><h5 class=card-title><a href="../../index.html">Wenda Chu 储闻达</a></h5><h6 class=card-subtitle>PhD Student</h6><p class=card-text>   </p><ul class=network-icon aria-hidden=true></ul></div></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href="index.html#" target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href="index.html#" target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src="../../js/vendor-bundle.min.b73dfaac3b6499dc997741748a7c3fe2.js"></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin=anonymous title=mermaid></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="https://chuwd19.github.io/archived_note/adversarial-ml/{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src="../../en/js/wowchemy.min.d68ecd57c0ec1f1f61d65fd568f1c3a0.js"></script></body></html>