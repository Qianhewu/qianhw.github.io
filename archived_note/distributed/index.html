<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Wowchemy 5.3.0 for Hugo"><meta name=google-site-verification content="Y6phf4VZM9HednfYy-pEi3-KtU6uqI75tPeeinWlOYs"><meta name=baidu-site-verification content="code-7BynQM9TAs"><meta name=author content="Wenda Chu 储闻达"><meta name=description content="My review notes on course Distributed System and BlockChain."><link rel=alternate hreflang=en-us href="index.html"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><meta name=theme-color content="#1565c0"><script src="../../js/mathjax-config.js"></script><link rel=stylesheet href="../../css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css" media=print onload="this.media='all'"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin=anonymous media=print onload="this.media='all'"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css crossorigin=anonymous title=hl-light media=print onload="this.media='all'"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css crossorigin=anonymous title=hl-dark media=print onload="this.media='all'" disabled><script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js integrity crossorigin=anonymous async></script><link rel=preload as=style href="https://fonts.googleapis.com/css2?family=Cutive+Mono&family=Lora:wght@400;700&family=Roboto:wght@400;700&display=swap&display=swap"><link rel=stylesheet href="https://fonts.googleapis.com/css2?family=Cutive+Mono&family=Lora:wght@400;700&family=Roboto:wght@400;700&display=swap&display=swap" media=print onload="this.media='all'"><link rel=stylesheet href="../../css/wowchemy.ed2da72110e60110d27da0704ff033d1.css"><link rel=manifest href="../../manifest.webmanifest"><link rel=icon type=image/png href="../../media/icon_hud9f11bce4f3a2a4889ae0de212996427_55561_32x32_fill_lanczos_center_2.png"><link rel=apple-touch-icon type=image/png href="../../media/icon_hud9f11bce4f3a2a4889ae0de212996427_55561_180x180_fill_lanczos_center_2.png"><link rel=canonical href="index.html"><meta property="twitter:card" content="summary"><meta property="og:site_name" content="Wenda Chu"><meta property="og:url" content="https://chuwd19.github.io/archived_note/distributed/"><meta property="og:title" content="Distributed System | Wenda Chu"><meta property="og:description" content="My review notes on course Distributed System and BlockChain."><meta property="og:image" content="https://chuwd19.github.io/media/icon_hud9f11bce4f3a2a4889ae0de212996427_55561_512x512_fill_lanczos_center_2.png"><meta property="twitter:image" content="https://chuwd19.github.io/media/icon_hud9f11bce4f3a2a4889ae0de212996427_55561_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2021-06-21T00:00:00+00:00"><meta property="article:modified_time" content="2021-06-21T00:00:00+00:00"><title>Distributed System | Wenda Chu</title></head><body id=top data-spy=scroll data-offset=70 data-target=#TableOfContents class=page-wrapper data-wc-page-id=1868a1f046b8cb87b4229fa4e5c31365><script src="../../js/wowchemy-init.min.8988fb2a4bba758785868cfcb5244555.js"></script><aside class=search-modal id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href="index.html#" aria-label=Close><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off spellcheck=false type=search class=form-control aria-label=Search...></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><div class=page-header><nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id=navbar-main><div class=container-xl><div class="d-none d-lg-inline-flex"><a class=navbar-brand href="../../index.html">Wenda Chu</a></div><button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar-content aria-controls=navbar-content aria-expanded=false aria-label="Toggle navigation">
<span><i class="fas fa-bars"></i></span></button><div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none"><a class=navbar-brand href="../../index.html">Wenda Chu</a></div><div class="navbar-collapse main-menu-item collapse justify-content-start" id=navbar-content><ul class="navbar-nav d-md-inline-flex"><li class=nav-item><a class=nav-link href="../../index.html#about"><span>Home</span></a></li><li class=nav-item><a class=nav-link href="../../index.html#featured"><span>Publications</span></a></li><li class=nav-item><a class=nav-link href="../../index.html#projects"><span>Projects</span></a></li><li class=nav-item><a class=nav-link href="../../index.html#contact"><span>Contact</span></a></li><li class=nav-item><a class=nav-link href="../../note.html"><span>Notes</span></a></li><li class=nav-item><a class=nav-link href="../../life.html"><span>Life</span></a></li><li class=nav-item><a class=nav-link href="../../uploads/resume.pdf"><span>CV</span></a></li></ul></div><ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2"><li class=nav-item><a class="nav-link js-search" href="index.html#" aria-label=Search><i class="fas fa-search" aria-hidden=true></i></a></li><li class="nav-item dropdown theme-dropdown"><a href="index.html#" class=nav-link data-toggle=dropdown aria-haspopup=true aria-label="Display preferences"><i class="fas fa-moon" aria-hidden=true></i></a><div class=dropdown-menu><a href="index.html#" class="dropdown-item js-set-theme-light"><span>Light</span></a>
<a href="index.html#" class="dropdown-item js-set-theme-dark"><span>Dark</span></a>
<a href="index.html#" class="dropdown-item js-set-theme-auto"><span>Automatic</span></a></div></li></ul></div></nav></div><div class=page-body><article class=article><div class="article-container pt-3"><h1>Distributed System</h1><p class=page-subtitle>My review notes on course Distributed System and BlockChain.</p><div class=article-metadata><div><span class=author-highlighted>Wenda Chu 储闻达</span></div><span class=article-date>Jun 21, 2021</span>
<span class=middot-divider></span><span class=article-categories><i class="fas fa-folder mr-1"></i><a href="../../category/notes/index.html">Notes</a></span></div><div class="btn-links mb-3"><a class="btn btn-outline-primary btn-page-header" href="Distributed.pdf" target=_blank rel=noopener>PDF</a></div></div><div class=article-container><div class=article-style><h2 id=centerreview---finalcenter><center>Review - Final</center></h2><h3 id=11-intro>1.1 Intro</h3><h5 id=characteristics-of-ds>Characteristics of DS</h5><ul><li><p>Present a single-system image</p><ul><li>Hide internal organization, communication details</li><li>Provide uniform interface</li></ul></li><li><p>Easily expandable</p><ul><li>Adding new servers is hidden from users</li></ul></li><li><p>Continuous availability</p><ul><li>Failures in one component can be covered by other components</li></ul></li><li><p>Supported by middleware</p></li></ul><h5 id=goal-of-ds>Goal of DS</h5><ul><li><p>Resource Availiability</p></li><li><p>Transparancy: hide details and appears to its users & applications to be a single computer system</p></li><li><p>Openness:</p><ul><li>Interoperability: The ability of two different systems or applications to work together</li><li>Portability: An application designed to run on one distributed system can run on another system which implements the same interface.</li><li>Extensibility: Easy to add new components, features</li></ul></li><li><p>Scalability: w.r.t. size, geographical distribution, number of administrative organizations spanned</p></li></ul><h3 id=12-classical-synchronization>1.2 Classical Synchronization</h3><h5 id=concurrency>Concurrency</h5><ul><li><p>Allows safe/multiplexed access to shared resources</p></li><li><p><strong>Critical Section</strong>: piece of code accessing a shared resource, usually variables or data structures</p></li><li><p><strong>Race Condition</strong>: Multiple threads of execution enter CS at the same time, update shared resource, leading to undesirable outcome</p></li><li><p><strong>Indeterminate Program</strong>: One or more Race Conditions, output of program depending on ordering, non-deterministic</p></li></ul><h5 id=mutual-exclusion>Mutual Exclusion</h5><ul><li><p>guarantee that only a single thread/process enters a CS, avoiding races</p></li><li><p><strong>Correctness</strong>: single process in CS at one time</p></li><li><p><strong>Efficiency</strong>: No waiting for availible resources, no spin-locks</p></li><li><p><strong>Bounded waiting</strong>: Fairness. No process waits forever.</p></li><li><p><strong>Atomic</strong> Test-and-set $\Longrightarrow$ Mutex</p></li></ul><pre><code class=language-go>Acquire_Mutex(&lt;mutex&gt;){while(!TestAndSet(&lt;mutex&gt;))}
{CS}
Release_Mutex(&lt;mutex&gt;){&lt;mutex&gt; = 1} 
</code></pre><ul><li>Semaphore: Initialized and set to integer value<ul><li>P(x) stands for proberen, Dutch for “to test”</li><li>V(x) stands for verhogen, Dutch for “to increment”</li><li>binary semaphore = mutex</li></ul></li></ul><pre><code class=language-Go>x.P():
	while (x == 0) wait; 
	x–-
x.V():
	x++ 
</code></pre><ul><li>Condition variables:<ul><li>cvars provide a sync point, one thread suspended until activated by another. (more efficient way to wait than spin lock )</li><li>cvar always associated with mutex</li><li><code>Wait()</code> and <code>Signal()</code> operations defined with cvars</li></ul></li></ul><p><img src="fig/cvars.png" alt=avatar style=zoom:50%></p><h5 id=example-fifo-queue>Example: FIFO queue</h5><pre><code class=language-go>b.Remove():
	b.mutex.lock()
	x = b.sb.Remove()
	b.mutex.unlock()
	return x
</code></pre><ul><li>Incorrect. If empty, lock forever</li></ul><pre><code>b.Remove():
	retry:
		b.mutex.lock()
		if !(b.sb.len() &gt; 0){
			b.mutex.unlock()
			goto retry	
		}
</code></pre><ul><li>This introduces a spin-lock, not efficient. Also may lead to a <strong>livelock</strong>.</li><li><strong>Livelock:</strong> Processes running without making progress.</li></ul><pre><code class=language-c>b.Init():
	b.sb = NewBuf()
	b.mutex = 1
	b.cvar = NewCond(b.mutex)
	
b.Insert(x):
	b.mutex.lock()
	b.sb.Insert(x)
	b.sb.Signal()
	b.mutex.unlock()
	
b.Remove():
	b.mutex.lock()
	while b.sb.Empty() {
		b.cvar.wait()
	}
	x = b.sb.Remove()
	b.mutex.unlock()
	return x

b.Flush():
   b.mutex.lock()
   b.sb.Flush() 
   b.mutex.unlock()
</code></pre><ul><li><p>Use while instead of if:</p><ul><li>With Mesa semantics, there is a point of vulnerability right after resuming execution and before locking mutex.</li><li>Hence, always recheck the condition using a while loop.</li></ul></li><li><p>Concurrency vs. Parellelism</p><ul><li>Concurrency is not parallelism, although it enables parallelism</li><li>1 Processor: Program can still be concurrent but not parallel</li></ul></li></ul><h3 id=2-networks>2 Networks</h3><h5 id=network-links>Network Links</h5><ul><li><p>Latency: first package to reach</p></li><li><p>Capacity (bandwidth): bits/sec</p></li><li><p>Jitter: Variation in latency</p></li><li><p>Loss/Reliability: Drop packages or not</p></li><li><p>Reordering</p></li><li><p>Package Delay:</p><ul><li>Propagation: Latency</li><li>Transimission: Bandwidth, depending on the bottleneck link</li><li>Processing: Router speed</li><li>Queueing: Traffic load and queue size</li><li>RTT: Round trip time = 2 $\times$ Latency</li></ul></li><li><p>Store and forward Protocol:</p><ul><li><strong>Store only one package instead of the full data!</strong></li><li>Propagation Delay + Transmission delay + Store and Forward delay(package size / arriving rate)</li></ul></li><li><p>Stop and wait Protocol:</p><ul><li>Send a single package and wait for acknowledgement</li><li>Improvement: Constantly sending packages and use a sliding window to record unacknowledged packages</li></ul></li></ul><h5 id=ethernet-frame>Ethernet Frame</h5><ul><li><p>Addresses: 6 bytes (MAC address)</p></li><li><p>Type: 2 bytes. Indicates the higher layer protocol, mostly IP.</p></li><li><p>Frame is received by all adapters on a LAN and dropped if address does not match.</p></li><li><p>When receiving a package, the bridge looks up the entry for the destiny MAC address</p><ul><li>If exists, forward</li><li>If no, boardcast except the arriving port</li></ul></li><li><p>Learning bridges: Fill in the forward table by source addresses</p></li></ul><h5 id=inter-net>Inter-net</h5><ul><li><p>Challenges: Heterogeneity</p></li><li><p>Need a standard: IP</p></li><li><p>IP address: DNS Translates human readable names to logical endpoints</p></li><li><p><strong>Connection with Link layer:</strong></p><ul><li><strong>ARP</strong> (Address Resolution Protocol): Transfer an IP address to a MAC address</li><li>Boardcast search, destination responses</li></ul></li><li><p><strong>Getting an IP address:</strong></p><ul><li><p>ISPs get from Regional Internet Registries (RIRs)</p></li><li><p>Or Dynamic Host Configuration Protocol (DHCP)</p></li></ul><p><img src="fig/DHCP.png" alt=avatar style=zoom:35%></p></li></ul><h5 id=layering>Layering</h5><ul><li><p>Example: Application $\Rightarrow$ Transport $\Rightarrow$ Network $\Rightarrow$ Link</p></li><li><p>Each layer relies on services from layer below and exports services to layer above</p></li><li><p><strong>Protocols</strong> define:</p><ul><li>Interface to higher layers (API)</li><li>Interface to peer (syntax & semantics)</li></ul></li><li><p>Hide implementation: Change layers without disturbing other layers</p></li></ul><h5 id=transport-protocols>Transport Protocols</h5><ul><li>Hop-by-hop vs. end-to-end</li><li>UDP vs. TCP</li><li>UDP: voice, multimedia</li><li>TCP: Web, Mails</li></ul><center><img style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" width=450 ; src="fig/socket.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#555;padding:2px">Web connection diagram</div></center><h3 id=31-synchronization>3.1 Synchronization</h3><h5 id=coordinated-universal-time-utc>Coordinated Universal Time (UTC)</h5><ul><li>Signals from land-based stations: 0.1-10 milliseconds ($ms$)</li><li>Signals from GPS: 1 microsecond ($\mu s$)</li><li>Clock drift rate: $10^{-6} sec/sec$</li><li><strong>Network Time Protocol (NTP)</strong>: hierarchical synchronization. Fits PC demand.</li></ul><h5 id=synchronization-algorithm>Synchronization Algorithm</h5><ul><li><p>Bound error by bounding propagation delay: set time to $T + D/2$</p></li><li><p><strong>Cristian&rsquo;s algorithm</strong></p><center><img style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" width=300 ; src="fig/Cristian.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#555;padding:2px">Cristian's algorithm</div></center><ul><li>Measures RTT $d$. Receiver set time to $T+ d/2$</li><li>Error bounded by $d/2$</li></ul></li><li><p><strong>Berkeley algorithm</strong></p><ul><li>One master clock send request to all others, compute the average and inform everyone to adjust</li></ul></li></ul><h3 id=32-distributed-logical-clocks>3.2 Distributed Logical Clocks</h3><h5 id=happens-before-relatioin>Happens Before relatioin</h5><ul><li><p>$a\to_i b$ if a is in front of b in $i$&rsquo;s' local event</p></li><li><p>$a\to b$ if $a$ is the event of sending message while $b$ is to receive it</p></li><li><p><strong>Concurrent events</strong>: $a|b$</p></li></ul><h5 id=lamport-clock>Lamport Clock</h5><ul><li>If $e \to e^\prime$, we must have $LC(e) &lt; LC(e^\prime)$</li><li>BUT not the reverse</li><li><strong>Lamport&rsquo;s algorithm</strong><ul><li>Local: increment $LC_i$ for each event</li><li>When receiving messages $(m,t)$, $LC_j = \max (LC_j,t)$</li><li>$LC(e) = LC_i(e)$</li></ul></li><li><strong>Total-order Lamport Clock:</strong><ul><li>$LC(e) = M \times LC_i(e) +i$</li><li>$M = # $ of processes</li></ul></li></ul><h5 id=vector-clock>Vector Clock</h5><ul><li>Label each event with $V(e)[c_1,\dots, c_n]$, where $c_i$ is the number of events in process i that causally precede e</li></ul><h6 id=remark>Remark:</h6><ul><li>Lamport clock provides one-way encoding from causality to logical time;</li><li>Vector clock provides exact causality information</li></ul><h3 id=4-blockchain>4 Blockchain</h3><h4 id=41-hash-functions>4.1 Hash Functions</h4><h5 id=collision-free>Collision-Free</h5><ul><li>computationally hard to find $x,y$, s.t. $x \neq y$ but $H(x) =H(y)$</li></ul><h5 id=hiding-one-way-function>Hiding (One-way function)</h5><ul><li>Given $H(x)$, hard to find $x$</li></ul><h5 id=puzzle-friendly>Puzzle-friendly</h5><ul><li>no solving strategy is much better than trying random values of $x$</li></ul><h5 id=sha-256>SHA-256</h5><center><img style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" width=450 ; src="fig/SHA.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#555;padding:2px">SHA</div></center><h5 id=blockchain>Blockchain</h5><ul><li><strong>Hash pointer:</strong> pointer to where the info is stored, and also the hash of the info</li><li>When modify one block, all the blocks after would know</li></ul><center><img style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" width=450 ; src="fig/BC.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#555;padding:2px">Blockchain</div></center><h5 id=merkle-tree>Merkle Tree</h5><ul><li>Use <strong>Hash pointers</strong> to form a tree. Data stored at the bottom.</li><li>$n$ data blocks requires $\log n$ layers. Show $\log n$ items to prove membership.</li></ul><h4 id=42-bitcoin-consensus>4.2 Bitcoin Consensus</h4><h5 id=consensus-algorithm>Consensus Algorithm</h5><ol><li>New transactions are broadcast to all nodes</li><li>Each node collects new transactions into a block</li><li>In each round a <strong>random</strong> node gets to broadcast its block</li><li>Other nodes accept the block only if all transactions in it are valid (unspent, valid signatures)</li><li>Nodes express their acceptance of the block by including its hash in the next block they create</li></ol><h6 id=remark-1>Remark:</h6><ul><li><p>Protection against invalid transactions is cryptographic, but enforced by consensus</p></li><li><p>Protection against double-spending is purely by consensus</p></li><li><p>Double spend probability decreases exponentially with # of confirmations</p></li></ul><h5 id=incentives>Incentives</h5><ul><li>Block reward</li><li>Transaction fees</li></ul><h5 id=randomness-of-creating-node>Randomness of creating node</h5><ul><li><p>Puzzle: $H(\text{nonce}| \text{prev_hash}| \text{data})$ is small</p></li><li><p>nonce published as part of the block</p></li></ul><h3 id=5-remote-procedure-call>5 Remote Procedure Call</h3><ul><li><strong>RPC</strong>: attempts to make remote procedure calls look like local ones</li></ul><h5 id=go-example>Go example:</h5><ul><li><strong>Client side</strong>: First dials the server, then make a remote call:</li></ul><pre><code class=language-go>client, err := rpc.DialHTTP(&quot;tcp&quot;, serverAddress + &quot;:1234&quot;)
if err != nil { log.Fatal(&quot;dialing:&quot;, err) }
args := &amp;server.Args{7,8}
var reply int
err = client.Call(&quot;Arith.Multiply&quot;, args, &amp;reply)
if err != nil {
	log.Fatal(&quot;arith error:&quot;, err)
}
fmt.Printf(&quot;Arith: %d*%d=%d&quot;, args.A, args.B, reply)

</code></pre><ul><li><h5 id=server-side>Server side:</h5></li></ul><pre><code class=language-go>package server
type Args struct { A, B int }
type Quotient struct { Quo, Rem int } 
type Arith int 
func (t *Arith) Multiply(args *Args, reply *int) error { 
*reply = args.A * args.B 
return nil } 
func (t *Arith) Divide(args *Args, quo *Quotient) error { 
	if args.B == 0 { return errors.New(&quot;divide by zero&quot;) } 
	quo.Quo = args.A / args.B 
	quo.Rem = args.A % args.B 
	return nil 
} 
</code></pre><ul><li><p>The server then calls (for HTTP service):</p><pre><code class=language-go>arith := new(Arith) 
rpc.Register(arith) 
rpc.HandleHTTP() 
l, e := net.Listen(&quot;tcp&quot;, &quot;:1234&quot;) 
if e != nil { log.Fatal(&quot;listen error:&quot;, e) } 
go http.Serve(l, nil)
</code></pre></li><li><p>Create a map from function name to functions:</p></li><li><p>for example, <code>Arith.Multiply</code> $\longrightarrow$ <code>&Multiply()</code></p></li><li><p>Messaging go objects:</p><ul><li>Marshal / Unmarshal; Serialization/Deserialization</li><li><strong>Marshal</strong>: Transfer structured objects to sequential text</li></ul></li></ul><p><strong>Stub</strong>: Obtaining transparency</p><ul><li>Client stub:<ul><li>Marshal arguments into machine independent format</li><li>unmarshals results received from server</li></ul></li><li>Server stub:<ul><li>unmarshals arguments and builds stack frame</li><li>calls procedure</li><li>marshals results and sends reply</li></ul></li></ul><h5 id=endian>Endian</h5><ul><li>An agreement on little or big endian: Network order</li></ul><h5 id=semantics-break-transparency>Semantics: Break transparency</h5><ul><li>Expose remoteness to client, since you cannot hide them (Cannot distinguish a failure from latency)</li><li>Exactly-once<ul><li>Impossible in practice</li><li>The robot could crash immediately before or after messaging and lose its state. Don’t know which one happened.</li></ul></li><li>At least once:<ul><li>Only for idempotent operations</li><li>Clients just keep trying unti getting a response</li><li>Server just processes requests as normal, doesn‘t remember anything. Simple!</li></ul></li><li>At most once<ul><li>Zero, don’t know, or once</li><li>Must re-send previous reply and not process request (implies: keep cache of handled requests/responses)</li><li>Must be able to identify requests</li><li>Solution: Keep sliding window of valid RPC IDs, have clients number them sequentially.</li></ul></li><li>Zero or once<ul><li>Transactional semantics</li></ul></li></ul><h5 id=asynchronized-rpc>Asynchronized RPC</h5><center><img style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" width=500 ; src="fig/Asyn.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#555;padding:2px">Asynchronized RPC</div></center><pre><code class=language-go>// Asynchronous call
quotient := new(Quotient)
divCall := client.Go(&quot;Arith.Divide&quot;, args, quotient, nil)
replyCall := &lt;-divCall.Done   // will be equal to divCall
// check errors, print, etc.
</code></pre><h3 id=6-mutual-exclusion>6 Mutual Exclusion</h3><h5 id=requirements>Requirements</h5><ul><li>Correctness: At most one process holds the lock</li><li>Fairness: no starvation</li><li>Low message overhead (protocol complexity)</li><li>Tolerate out-of-order messages</li></ul><h4 id=61-centralized-algorithm>6.1 Centralized Algorithm</h4><h5 id=coordinator>Coordinator:</h5><pre><code class=language-python>while true:      	
    m = Receive()      	
    if m == (Request, i)		
    	if Available():	    			
        	Send (Grant) to i   		
        else:            
            Put i in the queue    
    if m == (Release)&amp;&amp;!empty(Q):    	
        Remove ID j from Q	     	
        Send (Grant) to j
</code></pre><h5 id=clients>Clients:</h5><pre><code class=language-xml>Request:	
	Send (Request, i) to coordinator 	
	Wait for reply
Release:	
	Send (Release, i) to coordinator
</code></pre><ul><li>Correct and Fair (If clients never crash)!</li><li>Performance:<ul><li>3 cycles per cycle (1 request, 1 grant, 1 release)</li></ul></li></ul><h5 id=selecting-a-leader-bully-algorithm>Selecting a leader: bully algorithm</h5><h4 id=62-decentralized-algorithm>6.2 Decentralized Algorithm</h4><ul><li>Assume that there are $n$ coordinators<ul><li>Access requires a majority vote from $m > n/2$ coordinators.</li><li>A coordinator always responds immediately to a request with GRANT or DENY</li></ul></li><li>Node failures are still a problem<ul><li>Coordinators may forget vote on reboot</li></ul></li><li>What if you get less than $m$ votes?<ul><li>Backoff and retry later</li><li>Large numbers of nodes requesting access can affect availability</li><li>Starvation!</li></ul></li></ul><h4 id=63-totally-ordered-multicast>6.3 Totally Ordered Multicast</h4><ul><li>Use totally ordered Lamport clock</li><li>Details<ul><li>Each message is timestamped with the current logical time of its sender.</li><li>Assume all messages sent by one sender are received in the order they were sent and that no messages are lost.</li><li>Receiving process puts a message into a local queue ordered according to timestamp.</li><li>The receiver multicasts an ACK to all other processes.</li><li>Only deliver message when it is <em>both</em> at the head of queue and ack’ed by all participants</li></ul></li></ul><h4 id=64-distributed-mutual-exclusion>6.4 Distributed Mutual Exclusion</h4><h5 id=an-operation-to-cs-totally-ordered-multicast>An operation to CS: totally ordered Multicast</h5><ul><li><p><strong>Difference</strong></p><ul><li>the receiver only need to unicast the ack to its sender, since only the requester needs to know the message is ready to commit.</li><li>Release messages are broadcast to let others to move on</li></ul></li><li><p>Correctness</p><ul><li>When process x generates request with time stamp $T_x$, and it has received replies from all $y$ in $N_x$, then its $Q$ contains all requests with time stamps $\leq T_x$.</li></ul></li><li><p>Performance</p><ul><li>Process i sends $n-1$ request messages</li><li>Process i receives $n-1$ reply messages</li><li>Process i sends $n-1$ release messages.</li></ul></li></ul><h5 id=improvement-ricart--agrawala>Improvement: Ricart & Agrawala</h5><ul><li><p>Trick: Only reply after completing its own earlier operations in the CS</p></li><li><p>Deadlock free: since there is no cycles such that $T_a &lt; T_b &lt; \dots &lt; T_a$</p></li><li><p>Starvation free: after requesting with time stamp $T_a$, every other processes will update their clock to $> T_a$.</p></li><li><p>Performance: $n-1$ requests and $n-1$ replies.</p></li></ul><h5 id=a-token-ring-algorithm>A token ring algorithm</h5><ul><li>Correctness:<ul><li>Clearly safe: Only one process can hold token</li></ul></li><li>Fairness:<ul><li>Will pass around ring at most once before getting access.</li></ul></li><li>Performance:<ul><li>Each cycle requires between $1 - \infty$ messages</li><li>Latency of protocol between 0 & $n-1$</li></ul></li></ul><center><img style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" width=600 ; src="fig/Mutual&#32;Exclusion.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#555;padding:2px">Mutual Exclusion methods</div></center><h3 id=7-distributed-file-system>7 Distributed File System</h3><ul><li>Data sharing among multiple users</li><li>User mobility</li><li>Location transparency</li><li>Backups and centralized management</li></ul><h5 id=vfs>VFS</h5><center><img style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" width=600 ; src="fig/dfs.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#555;padding:2px">VFS</div></center><h5 id=a-simple-approach-nfs>A simple approach (NFS)</h5><ul><li>Use RPC to forward every file system operation to the server</li><li>Server serializes all accesses, performs them, and sends back result.</li><li>Great: Same behavior as if both programs were running on the same local filesystem!</li><li>Bad: Performance can stink. Latency of access to remote server often much higher than to local memory.</li></ul><h5 id=afs>AFS</h5><ul><li><p><strong>Assumptions</strong></p><ul><li>Clients can cache whole files over long periods</li><li>Write/Write, Write/Read share are rare</li></ul></li><li><p><strong>Cells and Volumes</strong></p><ul><li>cell: administrative groups</li><li>cells broken into volumes</li></ul></li></ul><h5 id=caching>Caching</h5><ul><li>NFS Write:<ul><li>Dirty data are buffered on the client machine until file close or up to 30 seconds</li><li>File attributes in the client cache expire after 60 seconds</li><li>when file is closed, all modified blocks sent to server.</li></ul></li><li>AFS<ul><li><strong>Callbacks:</strong> server tells clients &ldquo;Invalidate&rdquo; if the file changes. So the client may re-read it.</li><li><strong>Remove Callback</strong> when client has flushed the data from its disk</li></ul></li><li><strong>Tradeoff:</strong> consistency, performance, scalability.</li><li>Client-side caching is a fundamental technique to improve scalability and performance. But raises important questions of cache consistency.</li></ul><h5 id=name-space>Name Space</h5><ul><li>NFS: per-client linkage vs. AFS: global name space</li><li>NFS: no transparency<ul><li>If a directory is moved from one server to another, client must remount</li></ul></li><li>AFS: transparency<ul><li>If a volume is moved from one server to another, only the volume location database on the servers needs to be updated</li></ul></li></ul><h3 id=8-distributed-replication>8 Distributed Replication</h3><ul><li>Write replication requires some degree of consistency</li><li><strong>Strict Consistency</strong><ul><li>Read always returns value from latest write</li></ul></li><li><strong>Sequential Consistency</strong><ul><li>All nodes see operations in some sequential order</li><li>Operations of each process appear in-order in this sequence</li></ul></li></ul><center><img style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" width=700 ; src="fig/Sequential.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#555;padding:2px"></div></center><ul><li><p><strong>Causal Consistency</strong></p><center><img style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" width=700 ; src="fig/1.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#555;padding:2px"></div></center><ul><li><p><code>P1: W(x)c</code> and <code>P2: W(x)b</code> are concurrent so its not important that all processes see them in the same order<br>However <code>Wx(a)</code> and <code>R(x)a</code> and then <code>W(x)b</code> are potentially causally related so they must be in order.</p></li><li><p>This sequence is allowed with a causally-consistent store, but not with a sequentially consistent store.</p></li></ul></li></ul><h5 id=81-primary-backup-replication-model>8.1 Primary-backup Replication Model</h5><ul><li><p>Assumptions:</p><ul><li><p>Group membership manager: allow replica nodes to join/leave</p></li><li><p><strong>Fail-stop failure model:</strong> (not Byzantine) server may crash, might come up again.</p></li><li><p>Failure detector</p></li></ul></li><li><p>Primary backup: Writes always go to primary, read from any backup</p><center><img style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" width=700 ; src="fig/primary&#32;backup.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#555;padding:2px">parimary backup</div></center></li><li><p>At least once or at most once: Ack send back after Backup finish; or Ack send back only after commited logged at Primary</p></li><li><p><strong>Major drawback:</strong> Slow response times in case of failures.</p></li></ul><h5 id=82-consensus-replication-model>8.2 Consensus Replication Model</h5><p><strong>Quorum based consensus:</strong></p><ul><li><p>Designed to have fast response time even under failures</p></li><li><p>Operate as long as majority of machines is still alive</p></li><li><p>To handle $f$ failures, must have $2f + 1$ replicas</p></li><li><p>Major difference: you want replicated Write protocols so that you can write to multiple replicas instead of just one.</p></li></ul><p><strong>Paxos approach</strong>: on multiple servers reaching consensus on a single value.</p><ul><li><p><strong>Requirements:</strong></p><ul><li><strong>Correctness</strong>: Only a single value may be chosen. A machine never learns that a value has been chosen unless it really has been. The agreed value X has been proposed by some node</li><li><strong>Liveness</strong>: Some proposed value is eventually chosen. If a value is chosen, servers eventually learn about it</li><li><strong>Fault-tolerance</strong>: If less than $N/2$ nodes fail, the rest should reach agreement eventually</li><li>Note: Paxos sacrifices liveness in favor of correctness</li></ul></li><li><p>Synchronous DS: bounded amount of time node can take to process and respond to a request</p></li><li><p>Asynchronous DS: timeout is not perfect</p></li><li><p><strong>FLP Impossibility</strong></p><ul><li>It is impossible for a set of processors in an asynchronous system to agree on a binary value, even if only a single processor is subject to an unannounced failure.</li></ul></li><li><p>Proposers, Acceptors, Learners</p><center><img style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" width=500 ; src="fig/Paxos.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#555;padding:2px">Paxos</div></center></li><li><p>The key: once a proposal with value $v$ is chosen, all higher proposals must have value $v$, since $v$ remains the highest accepted value (It occupies $m>N/2$ servers).</p></li><li><p><strong>Remark</strong>: Only proposer knows chosen value (majority acccepted). No guarantee that proposer’s original value v is chosen by itself. Number $n$ is basically a Lamport clock, always unique $n$.</p></li></ul><h3 id=9-byzantine-fault-tolerance>9 Byzantine Fault Tolerance</h3><ul><li>Dependability implies the following:<ul><li>Availability: probability the system operates correctly at any given moment</li><li>Reliability: ability to run correctly for a long interval of time</li><li>Safety: failure to operate correctly does not lead to catastrophic failures</li><li>Maintainability: ability to “easily” repair a failed system</li></ul></li><li>BFT: Nodes may be malicious. Must agree on a value among benign nodes.</li><li>Quorum base:<ul><li>Any two quorums must intersect at least one honest node.</li><li>For liveness, the quorum size must be at most $N-f$.</li><li>$2(N-f) - N \geq f + 1$, so $N\geq 3f+1$.</li></ul></li></ul><h5 id=byzantine-agreement>Byzantine agreement</h5><ul><li>Phase 1: Each process sends its value to the other processes.<ul><li>Correct processes send the same (correct) value to all.</li><li>Faulty processes may send different values to each if desired (or no message).</li></ul></li><li>Phase 2: Each process uses the messages to create a vector of responses – must be a default value for missing messages.</li><li>Phase 3: Each process sends its vector to all other processes.</li><li>Phase 4: Each process the information received from every other process to do its computation.</li></ul><p><img src="https://chuwd19.github.io/home/qianhw/%E4%B8%8B%E8%BD%BD/hugo/content/post/fig/BFT.png" alt=avatar style=zoom:33%></p><h3 id=10-gfs--mapreduce>10 GFS & MapReduce</h3><ul><li>GFS is a distributed fault-tolerant file system</li></ul><h5 id=gfs-assumptions>GFS Assumptions</h5><ul><li>Small number of large files</li><li>Large streaming reads</li><li>Large, sequential writes that append</li><li>Concurrent appends by multiple clients<ul><li>For concurrency, only need to lock a small size of disk</li></ul></li></ul><center><img style="border-radius:.3125em;box-shadow:0 2px 4px rgba(34,36,38,.12),0 2px 10px rgba(34,36,38,8%)" width=700 ; src="fig/GFS.png"><br><div style="color:orange;border-bottom:1px solid #d9d9d9;display:inline-block;color:#555;padding:2px">GFS</div></center><ul><li><p>Client sends master: <code>read(file name, chunk index)</code></p></li><li><p>Master’s reply: <code>(chunk ID, chunk version number, locations of replicas)</code></p></li><li><p>Client sends “closest” chunkserver w/replica: <code>read(chunk ID, byte range)</code></p></li><li><p>Chunkserver replies with data</p></li></ul><h5 id=gfs-master-server>GFS Master Server</h5><ul><li><p>Holds all metadata:</p><ul><li>namespace</li><li>access control information</li><li>mapping from files to chunks</li><li>current locations of chunks</li></ul></li><li><p>Logs all client requests to disk sequentially</p></li><li><p>Replicates log entries to remote backup servers</p></li><li><p>Only replies to client after log entries safe on disk on self and backups!</p></li><li><p>Periodic checkpoints as an on-disk Btree</p></li></ul><h5 id=gfs-clients>GFS clients</h5><ul><li><p>Master grant lease to primary (for each chunk) (60 sec), which is renewed using periodic heartbeat</p></li><li><p>provide with 2 special operations:</p><ul><li><p>snapshot: creating a copy of the current instance of a file or directory tree.</p></li><li><p>append: allows clients to append data as an atomic operation without lock. Multiple processes can append to the same file concurrently</p></li></ul></li></ul><h5 id=fault-tolerant>Fault tolerant:</h5><ul><li><p>Master: Replays log from disk</p><ul><li>Recovers namespace (directory) information, recovers file-to-chunk-ID mapping (but not location of chunks)</li><li>Asks chunkservers which chunks they hold, recovers chunk-ID-to-chunkserver mapping</li><li>If chunk server has older chunk, it’s stale; if chunk server has newer chunk, adopt its version number</li></ul></li><li><p>Chunkserver dead:</p><ul><li>Master notices missing heartbeats, decrements count of replicas for all chunks on dead chunkserver</li><li>Master re-replicates chunks missing replicas in background</li></ul></li></ul><h5 id=mapreduce>MapReduce</h5><ul><li><p>Programs implement <code>Mapper</code> and <code>Reducer</code> classes</p></li><li><p><strong>Mapper</strong>: Generate <code>&lt;key,value></code> pairs</p></li><li><p><strong>Reducer</strong>: Iterate among all keys, outputs one or multiple <code>&lt;key,value></code> pairs</p></li><li><p>Remarks:</p><ul><li>Computation broken into many, short-lived tasks</li><li>Use disk storage to hold intermediate results</li></ul></li><li><p>Limitations: spend too much time on I/O to disks and over network. This makes interactive data analysis impossible</p></li></ul><h3 id=11-sparks>11 Sparks</h3><ul><li><p>In memory fault-tolerant computation</p></li><li><p><strong>Resilient Distributed Dataset</strong> (RDD)</p><ul><li><strong>Immutable</strong>: cannot be modified once created. This enables <strong>lineage</strong> (recreate any RDD at any time) and is compatiable with HDFS (append only).</li><li><strong>Transformations</strong>: create new RDD from existing ones</li><li><strong>Actions</strong>: compute a value based on an RDD. Either return or saved to an external storage system</li><li><strong>Persist</strong> RDD to a memory</li></ul></li><li><p>Transformations are lazy: their result RDD is not immediately computed. Their evaluation only triggered by Action!</p></li><li><p>This enables spark to optimize the required operations; and allows Spark to recover from failures and slow workers</p></li><li><p>By default, RDDs are recomputed each time you run an action on them. This can be expensive if you need to use the dataset more than once. Call <code>persist()</code> or <code>cache()</code> to cache an RDD in memory.</p></li><li><p>BSP computation abstraction: Any distributed system can be emulated as local work + message passing (=BSP)</p></li><li><p>Challenges: communication overheads and stragglers</p></li><li><p>P2P+selective communication, bounded-delay BSP</p></li></ul><h3 id=12-mining-pools-and-bitcoin>12 Mining Pools and Bitcoin</h3><h5 id=121-mining-pools>12.1 Mining pools</h5><ul><li>Partial Method used as measuring the amount of work a miner does</li><li>Naive solution: assign reward proportional to the amount of work.</li><li>Issue: If miners jump to new pools?<ul><li>The expected rewards: $\alpha_i\to \alpha_i + \text{old pool revenue}$</li></ul></li><li>Do not reward each share equally!</li></ul><p>Examples:</p><ul><li>Slush&rsquo;s method: scoring function: $s = e^{T/C}$. Gives advantage to miners who joined late.</li><li>Pay-per-share: the operator pays per each partial solution no matter if he managed to extend the chain.</li></ul><p>Attacks:</p><ul><li>Sabotage: Only submit partial solutions</li><li>Lie-in-wait: spread computing power over many pools. Once find one, wait a while only mining for that pool and then submit</li></ul><h5 id=122-bitcoin-transactions>12.2 Bitcoin Transactions</h5><ul><li><p>In: where do you get your money?</p><ul><li><code>prev_out</code>: previous transaction（收入来源的交易账单）only hash + index (since there may be multiple out)</li><li><code>scriptSig</code>: your signature</li></ul></li><li><p>Out:</p><ul><li><code>value</code>: how much you spend</li><li><code>scriptPubKey</code>: public key of acceptor</li><li>The rest coins must be sent back to yourself</li></ul></li><li><p>If tracing back each transaction, must end up with <code>coinbase</code>, which is generated by mining.</p></li><li><p><code>coinbase</code> has <code>prev_out: hash = 0, n = 4294967295</code>.</p></li><li><p>Multisig: specify $n$ public keys, verification requires $t$ signatures.</p></li><li><p>Example: 2-of-3 multisig used for escrow transactions.</p><ul><li>If either Alice or Bob does not fulfill his/her job, the third party (randomly selected) will give signature</li></ul></li><li><p>Pay to script hash (P2SH): the previous Pay to PublicKey Hash (P2PKH) is too complicated. The seller can design a script beforehead, so the buyer only need to send bitcoins to that hash address.</p></li><li><p>Lock time: designed for small transactions</p></li></ul><h5 id=123-limitation-and-improment>12.3 Limitation and Improment</h5><ul><li>throughput limitation: 7 transactions/sec, comparing to 2000-10000 for VISA</li><li>Hard-forking vs. soft-forking</li></ul></div><div class=article-tags><a class="badge badge-light" href="../../tag/typed/index.html">typed</a></div><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://chuwd19.github.io/archived_note/distributed/&text=Distributed%20System" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://chuwd19.github.io/archived_note/distributed/&t=Distributed%20System" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook"></i></a></li><li><a href="mailto:?subject=Distributed%20System&body=https://chuwd19.github.io/archived_note/distributed/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://chuwd19.github.io/archived_note/distributed/&title=Distributed%20System" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="whatsapp://send?text=Distributed%20System%20https://chuwd19.github.io/archived_note/distributed/" target=_blank rel=noopener class=share-btn-whatsapp><i class="fab fa-whatsapp"></i></a></li><li><a href="https://service.weibo.com/share/share.php?url=https://chuwd19.github.io/archived_note/distributed/&title=Distributed%20System" target=_blank rel=noopener class=share-btn-weibo><i class="fab fa-weibo"></i></a></li></ul></div><div class="media author-card content-widget-hr"><a href="../../index.html"><img class="avatar mr-3 avatar-circle" src="../../authors/admin/avatar_huec040978cd197f91fa586c800703b8ed_65970_270x270_fill_q75_lanczos_center.jpg" alt="Wenda Chu 储闻达"></a><div class=media-body><h5 class=card-title><a href="../../index.html">Wenda Chu 储闻达</a></h5><h6 class=card-subtitle>Undergraduate Student</h6><p class=card-text>I like chess, piano, animation and photography.</p><ul class=network-icon aria-hidden=true></ul></div></div></div></article></div><div class=page-footer><div class=container><footer class=site-footer><p class=powered-by>Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target=_blank rel=noopener>Wowchemy</a> — the free, <a href=https://github.com/wowchemy/wowchemy-hugo-modules target=_blank rel=noopener>open source</a> website builder that empowers creators.</p></footer></div></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href="index.html#" target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href="index.html#" target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div><script src="../../js/vendor-bundle.min.b73dfaac3b6499dc997741748a7c3fe2.js"></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mermaid/8.8.4/mermaid.min.js integrity="sha512-as1BF4+iHZ3BVO6LLDQ7zrbvTXM+c/1iZ1qII/c3c4L8Rn5tHLpFUtpaEtBNS92f+xGsCzsD7b62XP3XYap6oA==" crossorigin=anonymous title=mermaid></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js crossorigin=anonymous></script><script id=search-hit-fuse-template type=text/x-template>
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="https://chuwd19.github.io/archived_note/distributed/{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script><script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script><script src="../../en/js/wowchemy.min.d68ecd57c0ec1f1f61d65fd568f1c3a0.js"></script></body></html>